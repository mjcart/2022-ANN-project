{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1caff414",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c993de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version:  1.0.2\n",
      "TF version:  2.7.0\n",
      "GPU installed:  True\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "print(\"TF version: \", tf.__version__)\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# GPU test\n",
    "print(\"GPU installed: \",tf.test.is_built_with_gpu_support())\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")    \n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd569af",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48bdd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#작성자 전민재\n",
    "import csv\n",
    "def load_Emist(exsitNumpy=False, needTranspose=True):\n",
    "    if (exsitNumpy == False):\n",
    "        #\"./emnist-byclass-test.csv\"\n",
    "        #train-set\n",
    "        #\"공용/datasets/Emnist/\" 위치에 csv 저장 \n",
    "        csv_train_data_file = open(\"../공용/datasets/Emnist/train_data.csv\", \"r\", encoding=\"ms932\", errors=\"\", newline=\"\" ) #리스트 형식 \n",
    "        csv_train_label_file = open(\"../공용/datasets/Emnist/train_label.csv\", \"r\", encoding=\"ms932\", errors=\"\", newline=\"\" ) #리스트 형식 \n",
    "    \n",
    "        \n",
    "        f_train = csv.reader(csv_train_data_file, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "        l_train=csv.reader(csv_train_label_file, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "        #valid-set\n",
    "    \n",
    "        csv_valid_data_file = open(\"../공용/datasets/Emnist/valid_data.csv\", \"r\", encoding=\"ms932\", errors=\"\", newline=\"\" ) #리스트 형식 \n",
    "        csv_valid_label_file = open(\"../공용/datasets/Emnist/valid_label.csv\", \"r\", encoding=\"ms932\", errors=\"\", newline=\"\" ) #리스트 형식 \n",
    "        \n",
    "        f_valid = csv.reader(csv_valid_data_file, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "        l_valid = csv.reader(csv_valid_label_file, delimiter=\",\", doublequote=True, lineterminator=\"\\r\\n\", quotechar='\"', skipinitialspace=True)\n",
    "        X_train=[]\n",
    "        y_train=[]\n",
    "        X_valid=[]\n",
    "        y_valid=[]\n",
    "        for i, row in enumerate(f_train):\n",
    "            #행마다 int로 형변환\n",
    "            for idx, char in enumerate(row):\n",
    "                row[idx]=int(char)\n",
    "            #train\n",
    "            #data 추가    \n",
    "            X_train.append(row)\n",
    "            \n",
    "        for i, row in enumerate(f_valid):\n",
    "            #행마다 int로 형변환\n",
    "            for idx, char in enumerate(row):\n",
    "                row[idx]=int(char)\n",
    "            #valid\n",
    "            #data 추가    \n",
    "            X_valid.append(row)\n",
    "\n",
    "\n",
    "        for i, row in enumerate(l_train):\n",
    "\n",
    "            #train\n",
    "            #label 추가\n",
    "            y_train.append(int(row[0]))\n",
    "            \n",
    "        for i, row in enumerate(l_valid):\n",
    "\n",
    "            #valid\n",
    "            #label 추가\n",
    "            y_valid.append(int(row[0]))\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        #train\n",
    "        X_train=np.array(X_train,dtype=np.uint8)\n",
    "        X_train=X_train.reshape(-1,28,28)\n",
    "        #valid\n",
    "        X_valid=np.array(X_valid,dtype=np.uint8)\n",
    "        X_valid=X_valid.reshape(-1,28,28)\n",
    "        \n",
    "        csv_train_data_file.close()\n",
    "        csv_train_label_file.close()\n",
    "        csv_valid_data_file.close()\n",
    "        csv_valid_label_file.close()\n",
    "        \n",
    "        #kaggle dataset이 시계반대방향으로 90도 회전 되있고 상하 반전 되어있음\n",
    "        def rotate_90(m):\n",
    "            N = len(m)\n",
    "            ret = [[0] * N for _ in range(N)]\n",
    "\n",
    "            for r in range(N):\n",
    "                for c in range(N):\n",
    "                    ret[c][N-1-r] = m[r][c]\n",
    "            return np.array(ret,dtype=np.uint8)\n",
    "\n",
    "        def vreflect(m):\n",
    "            N = len(m)\n",
    "            ret = [[0] * N for _ in range(N)]\n",
    "\n",
    "            for r in range(N):\n",
    "                for c in range(N):\n",
    "                    ret[r][c] = m[r][N-1-c]\n",
    "            return np.array(ret,dtype=np.uint8)\n",
    "        \n",
    "        if needTranspose == True:\n",
    "            #train\n",
    "            #회전\n",
    "            for idx,i in enumerate(X_train):\n",
    "                X_train[idx]=rotate_90(i)\n",
    "            #상하반전\n",
    "            for idx,i in enumerate(X_train):\n",
    "                X_train[idx]=vreflect(i)\n",
    "\n",
    "            #valid\n",
    "            #회전\n",
    "            for idx,i in enumerate(X_valid):\n",
    "                X_valid[idx]=rotate_90(i)\n",
    "            #상하반전\n",
    "            for idx,i in enumerate(X_valid):\n",
    "                X_valid[idx]=vreflect(i)\n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        #저장용\n",
    "        #train\n",
    "        #(533917,28,28)\n",
    "        \n",
    "        np.save('../공용/X_train',np.array(X_train,dtype=np.uint8))\n",
    "        np.save('../공용/y_train',np.array(y_train,dtype=np.uint8))\n",
    "        #valid\n",
    "        np.save('../공용/X_valid',np.array(X_valid,dtype=np.uint8))\n",
    "        np.save('../공용/y_valid',np.array(y_valid,dtype=np.uint8))\n",
    "        \n",
    "   \n",
    "    X_train=np.load('../공용/X_train.npy')\n",
    "    y_train=np.load('../공용/y_train.npy').astype(np.uint8)\n",
    "    X_valid=np.load('../공용/X_valid.npy')\n",
    "    y_valid=np.load('../공용/y_valid.npy').astype(np.uint8)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586a914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((533917, 28, 28), dtype('uint8'), (533917,), dtype('uint8'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#황성현\n",
    "#npy 파일이 존재할 경우 exsitNumpy = True, 업으면 False\n",
    "X_train, y_train, X_valid, y_valid = load_Emist(exsitNumpy=True)\n",
    "X_train.shape,X_train.dtype,y_train.shape,y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55d2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_letters = X_train[np.where(y_train>10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad6cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_letters = y_train[np.where(y_train>10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5170faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (3):\n",
    "    X_train = np.append(X_train,X_train_letters,axis=0)\n",
    "    y_train = np.append(y_train, y_train_letters,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8099a952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1329679, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480f4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_letters, y_train_letters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ebce286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "ids=1\n",
    "X_new=X_train[ids]\n",
    "y_new=y_train[ids]\n",
    "c=X_new.reshape(28,28)\n",
    "\n",
    "\n",
    "plt.imshow(c, cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7fd29",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da90557f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1329679"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#황성현\n",
    "#data, batch size 성정\n",
    "train_size=len(X_train)\n",
    "valid_size=len(X_valid)\n",
    "batch_size=32\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b678435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1329679, 784), (164015, 784))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#황성현\n",
    "X_train=np.reshape(X_train,[-1,784])\n",
    "X_valid=np.reshape(X_valid,[-1,784])\n",
    "X_train.shape,X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1c2321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1329679, 1), (164015, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#황성현\n",
    "y_train=np.reshape(y_train,[-1,1])\n",
    "y_valid=np.reshape(y_valid,[-1,1])\n",
    "\n",
    "y_train.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76ee4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_inputs = 784\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X_train.shape[-1]\n",
    "print(\"n_inputs =\",n_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ce0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10, rewrite=False):\n",
    "    \n",
    "    Emnist_dir = os.path.join(\"../공용/datasets\", \"Emnist\")\n",
    "    os.makedirs(Emnist_dir, exist_ok=True)\n",
    "    path_format = os.path.join(Emnist_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        if rewrite:\n",
    "            \n",
    "            try:\n",
    "                with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "                    if header is not None:\n",
    "                        f.write(header)\n",
    "                        f.write(\"\\n\")\n",
    "                    for row_idx in row_indices:\n",
    "                        f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                        f.write(\"\\n\")\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                with open(part_csv, \"xt\", encoding=\"utf-8\") as f:\n",
    "                    if header is not None:\n",
    "                        f.write(header)\n",
    "                        f.write(\"\\n\")\n",
    "                    for row_idx in row_indices:\n",
    "                        f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                        f.write(\"\\n\")\n",
    "            except:\n",
    "                continue\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "258b4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full=np.append(X_train,y_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20204834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1329679, 785)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae33894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full[1020][784]==y_train[1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e708c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "train_filepaths = save_to_multiple_csv_files(train_full, \"train\", n_parts=20,rewrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72e06854",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_full=np.append(X_valid,y_valid,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9af778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_full, \"valid\", n_parts=20,rewrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e05ee648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2896"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#황성현\n",
    "#메모리 해제\n",
    "import gc\n",
    "del valid_full, train_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25aecc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "\n",
    "def dataaugmentation(X):\n",
    "    datagen=ImageDataGenerator(rotation_range=40,width_shift_range = 0.2, \n",
    "        height_shift_range = 0.2)\n",
    "    #영어 부분 2배 생성\n",
    "    #차원변환 \n",
    "    X=tf.reshape(X,[28,28,1])\n",
    "\n",
    "    #새로운 데이터 생성\n",
    "    angle=(random.random()%1)*0.8 - 0.4\n",
    "    image_result =tfa.image.rotate(X, tf.constant(angle))#rotate -pi/10 ~ pi/10\n",
    "    shift=(random.random()%1)*2 - 1\n",
    "    image_result=tfa.image.translate(image_result,[tf.constant(shift),tf.constant(shift)])\n",
    "    \n",
    "    return image_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bac0fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "@tf.function\n",
    "def preprocess_mobilenet_v2(line,randomize=False):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    x=tf.reshape(x,[28,28,1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    augment_image = x\n",
    "    if y > 10 and randomize:\n",
    "        augment_image = dataaugmentation(x)\n",
    "        \n",
    "        \n",
    "    resized_image = tf.image.resize(augment_image, [224, 224])\n",
    "    if resized_image.shape[2] == 1:\n",
    "        temp=tf.concat([resized_image,resized_image],2)\n",
    "        resized_image=tf.concat([temp,resized_image],2)\n",
    "    final_image = keras.applications.mobilenet_v2.preprocess_input(resized_image)\n",
    "    \n",
    "    return final_image, y\n",
    "\n",
    "@tf.function\n",
    "def preprocess_resnet50(line,randomize=False):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    x=tf.reshape(x,[28,28,1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    augment_image = x\n",
    "    if y > 10 and randomize:\n",
    "        augment_image = dataaugmentation(x)\n",
    "        \n",
    "        \n",
    "    resized_image = tf.image.resize(augment_image, [224, 224])\n",
    "    if resized_image.shape[2] == 1:\n",
    "        temp=tf.concat([resized_image,resized_image],2)\n",
    "        resized_image=tf.concat([temp,resized_image],2)\n",
    "    final_image = keras.applications.resnet50.preprocess_input(resized_image)\n",
    "    \n",
    "    return final_image, y\n",
    "\n",
    "@tf.function\n",
    "def preprocess_vgg16(line,randomize=False):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    x=tf.reshape(x,[28,28,1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    augment_image = x\n",
    "    if y > 10 and randomize:\n",
    "        augment_image = dataaugmentation(x)\n",
    "        \n",
    "        \n",
    "    resized_image = tf.image.resize(augment_image, [224, 224])\n",
    "    if resized_image.shape[2] == 1:\n",
    "        temp=tf.concat([resized_image,resized_image],2)\n",
    "        resized_image=tf.concat([temp,resized_image],2)\n",
    "    final_image = keras.applications.vgg16.preprocess_input(resized_image)\n",
    "    \n",
    "    return final_image, y\n",
    "\n",
    "@tf.function\n",
    "def preprocess_renet(line,randomize=False):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    x=tf.reshape(x,[28,28,1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    augment_image = x\n",
    "    if y > 10 and randomize:\n",
    "        augment_image = dataaugmentation(x)\n",
    "        \n",
    "   \n",
    "    return augment_image, y\n",
    "\n",
    "@tf.function\n",
    "def preprocess_xception(line,randomize=False):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    x=tf.reshape(x,[28,28,1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    augment_image = x\n",
    "    if y > 10 and randomize:\n",
    "        augment_image = dataaugmentation(x)\n",
    "        \n",
    "        \n",
    "    resized_image = tf.image.resize(augment_image, [224, 224])\n",
    "    if resized_image.shape[2] == 1:\n",
    "        temp=tf.concat([resized_image,resized_image],2)\n",
    "        resized_image=tf.concat([temp,resized_image],2)\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    \n",
    "    return final_image, y\n",
    "\n",
    "@tf.function\n",
    "def preprocess_inception_v3(line,randomize=False):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    x=tf.reshape(x,[28,28,1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    augment_image = x\n",
    "    if y > 10 and randomize:\n",
    "        augment_image = dataaugmentation(x)\n",
    "        \n",
    "        \n",
    "    resized_image = tf.image.resize(augment_image, [224, 224])\n",
    "    if resized_image.shape[2] == 1:\n",
    "        temp=tf.concat([resized_image,resized_image],2)\n",
    "        resized_image=tf.concat([temp,resized_image],2)\n",
    "    final_image = keras.applications.inception_v3.preprocess_input(resized_image)\n",
    "    \n",
    "    return final_image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98239594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "from enum import Enum\n",
    "class eModelName(Enum):\n",
    "    mobilenet_v2 = 0,\n",
    "    resnet50 = 1,\n",
    "    vgg16 = 3,\n",
    "    renet = 4,\n",
    "    xception = 5,\n",
    "    inception_v3 = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c1e75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "from functools import partial\n",
    "def csv_reader_dataset(filepaths, model_name, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32,randomize=True):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    if model_name == eModelName.mobilenet_v2:\n",
    "        dataset = dataset.map(partial(preprocess_mobilenet_v2,randomize=randomize), num_parallel_calls=n_parse_threads)\n",
    "        print(model_name.name)\n",
    "    elif model_name == eModelName.resnet50:\n",
    "        dataset = dataset.map(partial(preprocess_resnet50,randomize=randomize), num_parallel_calls=n_parse_threads)\n",
    "        print(model_name.name)\n",
    "    elif model_name == eModelName.vgg16:\n",
    "        dataset = dataset.map(partial(preprocess_vgg16,randomize=randomize), num_parallel_calls=n_parse_threads)\n",
    "        print(model_name.name)\n",
    "    elif model_name == eModelName.renet:\n",
    "        dataset = dataset.map(partial(preprocess_renet,randomize=randomize), num_parallel_calls=n_parse_threads)\n",
    "        print(model_name.name)\n",
    "    elif model_name == eModelName.xception:\n",
    "        dataset = dataset.map(partial(preprocess_xception,randomize=randomize), num_parallel_calls=n_parse_threads)\n",
    "        print(model_name.name)\n",
    "    elif model_name == eModelName.inception_v3:\n",
    "        dataset = dataset.map(partial(preprocess_inception_v3,randomize=randomize), num_parallel_calls=n_parse_threads)\n",
    "        print(model_name.name)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a12eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet_v2\n",
      "mobilenet_v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#황성현\n",
    "#csv_reader_dataset의 파라미터 model_name에 eModelNAme class의 맴버 변수 중 사용할 모델 입력\n",
    "#ex) model_name = eModelName.inception_v3, model_name = eModelName.renet \n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "batch_size=8\n",
    "train_set = csv_reader_dataset(train_filepaths, model_name = eModelName.mobilenet_v2, batch_size=batch_size, \n",
    "                               repeat=None,n_read_threads=tf.data.experimental.AUTOTUNE)\n",
    "valid_set = csv_reader_dataset(valid_filepaths, model_name = eModelName.mobilenet_v2, batch_size=batch_size, \n",
    "                               n_read_threads=tf.data.experimental.AUTOTUNE, repeat=None, randomize = False)\n",
    "train_set,valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95feced5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 28, 28), (1000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw=X_train[:1000]\n",
    "y_train_raw=y_train[:1000]\n",
    "X_train_raw.shape,y_train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851a1854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 224, 224, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resize=np.resize(X_train,[1000,224,224,1])\n",
    "X_train_resize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3357547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 224, 224, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stack=X_train_resize\n",
    "X_train_stack=np.append(X_train_stack,X_train_resize,axis=3)\n",
    "X_train_stack=np.append(X_train_stack,X_train_resize,axis=3)\n",
    "X_train_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f470551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 224, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = keras.applications.mobilenet_v2.preprocess_input(X_train_stack)\n",
    "X_train_final.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2834c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final=np.reshape(y_train_raw,[-1])\n",
    "y_train_final[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c2a2b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3ae77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2_mobilenet_v2(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    x=tf.reshape(x,[28,28,1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    print (y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b54f2",
   "metadata": {},
   "source": [
    "# SMV2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "069ccbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현\n",
    "class Inveted_Residual_Block(keras.layers.Layer):\n",
    "    def __init__(self,c,s=1,t=1,n=1,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.layers.ReLU(max_value=6)\n",
    "        self.main_layers = [\n",
    "            \n",
    "            ]\n",
    "        self.strides_layers=[\n",
    "           ]\n",
    "        self.s=s\n",
    "        self.t=t\n",
    "        self.c=c\n",
    "        self.n=n\n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        print(self.name,\": batch_input_shape =\",batch_input_shape)\n",
    "        \n",
    "        self.main_layers = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=batch_input_shape[-1]*self.t, kernel_size=1,strides=1, \n",
    "                                padding=\"SAME\", activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3, strides=1,padding=\"SAME\", activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=self.c,kernel_size=1, strides=1,padding=\"SAME\", activation=self.activation),\n",
    "            ]\n",
    "        self.strides_layers=[\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=batch_input_shape[-1]*self.t, kernel_size=1,strides=1,\n",
    "                                padding=\"SAME\", activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3, strides=self.s,padding=\"SAME\",activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=self.c, kernel_size=1,strides=1,padding=\"SAME\",activation=self.activation),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        \n",
    "        for n in range(0,self.n):\n",
    "            if n == 0:\n",
    "                for layer in self.strides_layers:\n",
    "                    Z = layer(Z)\n",
    "            else:\n",
    "                skip_Z = Z\n",
    "                for layer in self.main_layers:\n",
    "                    Z = layer(Z)\n",
    "                \n",
    "                Z += skip_Z\n",
    "    \n",
    "        return Z\n",
    "    \n",
    "    def get_config(self):\n",
    "        base=super().get_config()\n",
    "        return{**base,\"s\":self.s,\"t\":self.t,\"c\":self.c,\"n\":self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e091a132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 256, 256, 32)\n",
      "IRB2 : batch_input_shape = (None, 256, 256, 16)\n",
      "IRB3 : batch_input_shape = (None, 128, 128, 16)\n",
      "IRB4 : batch_input_shape = (None, 64, 64, 32)\n",
      "IRB5 : batch_input_shape = (None, 32, 32, 64)\n",
      "IRB6 : batch_input_shape = (None, 16, 16, 128)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 512, 512, 3)      12        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 256, 256, 32)      896       \n",
      "                                                                 \n",
      " IRB1 (Inveted_Residual_Bloc  (None, 256, 256, 16)     2288      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB2 (Inveted_Residual_Bloc  (None, 128, 128, 16)     6688      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB3 (Inveted_Residual_Bloc  (None, 64, 64, 32)       19456     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB4 (Inveted_Residual_Bloc  (None, 32, 32, 64)       67584     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB5 (Inveted_Residual_Bloc  (None, 16, 16, 128)      108416    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB6 (Inveted_Residual_Bloc  (None, 16, 16, 1024)     1200640   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 16, 16, 1024)      0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 62)                63550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,473,626\n",
      "Trainable params: 1,460,916\n",
      "Non-trainable params: 12,710\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #황성현\n",
    "        #multygpu\n",
    "    n_classes=62\n",
    "    smv2_l=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[512,512,3]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=4,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=8,c=32,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=8,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=8,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_l.summary()\n",
    "    smv2_l.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cebe24b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "166209/166209 [==============================] - 15640s 94ms/step - loss: 0.4624 - accuracy: 0.8444 - val_loss: 1.2096 - val_accuracy: 0.7441 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "166209/166209 [==============================] - 15351s 92ms/step - loss: 0.4095 - accuracy: 0.8577 - val_loss: 1.5769 - val_accuracy: 0.7196 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "166209/166209 [==============================] - 15351s 92ms/step - loss: 0.4364 - accuracy: 0.8505 - val_loss: 1.6969 - val_accuracy: 0.7011 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "166209/166209 [==============================] - 15469s 93ms/step - loss: 0.5126 - accuracy: 0.8291 - val_loss: 1.2431 - val_accuracy: 0.7450 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "166209/166209 [==============================] - 15272s 92ms/step - loss: 0.4062 - accuracy: 0.8589 - val_loss: 1.7443 - val_accuracy: 0.7060 - lr: 5.0000e-04\n",
      "Epoch 6/300\n",
      "166209/166209 [==============================] - 15264s 92ms/step - loss: 0.4804 - accuracy: 0.8385 - val_loss: 1.6281 - val_accuracy: 0.7158 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#황성현 \n",
    "from datetime import datetime\n",
    "logs = \"logs/\" + datetime.now().strftime(\"SMV2L_BATCH8%Y%m%d-%H%M%S\")\n",
    "\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2l_nadam_batch8_best.h5\",save_best_only=True)\n",
    "check_last_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2l_nadam_batch8_last.h5\",save_best_only=False)\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True)\n",
    "lr_cb = keras.callbacks.ReduceLROnPlateau(patience = 3, factor = 0.5 )\n",
    "history=smv2_l.fit(train_set, batch_size = batch_size,\n",
    "                    steps_per_epoch=int(train_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(valid_size / batch_size),\n",
    "                    epochs=300,callbacks=[tboard_callback, check_best_cb, check_last_cb, earlystop_cb,lr_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "547a6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 112, 112, 32)\n",
      "IRB2 : batch_input_shape = (None, 112, 112, 16)\n",
      "IRB3 : batch_input_shape = (None, 56, 56, 32)\n",
      "IRB4 : batch_input_shape = (None, 28, 28, 64)\n",
      "IRB5 : batch_input_shape = (None, 14, 14, 64)\n",
      "IRB6 : batch_input_shape = (None, 7, 7, 128)\n"
     ]
    }
   ],
   "source": [
    "smv2_p=keras.models.load_model(\"./smv2p_nadam_best.h5\",custom_objects={\"Inveted_Residual_Block\":Inveted_Residual_Block})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fa6d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 112, 112, 32)\n",
      "IRB2 : batch_input_shape = (None, 112, 112, 16)\n",
      "IRB3 : batch_input_shape = (None, 56, 56, 16)\n",
      "IRB4 : batch_input_shape = (None, 28, 28, 32)\n",
      "IRB5 : batch_input_shape = (None, 14, 14, 64)\n",
      "IRB6 : batch_input_shape = (None, 7, 7, 128)\n"
     ]
    }
   ],
   "source": [
    "smv2_l=keras.models.load_model(\"smv2l_nadam_batch8_best.h5\",custom_objects={\"Inveted_Residual_Block\":Inveted_Residual_Block})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac63bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9568/20501 [=============>................] - ETA: 3:22 - loss: 1.1977 - accuracy: 0.7465"
     ]
    }
   ],
   "source": [
    "smv2_l.evaluate(valid_set,steps=int(valid_size / batch_size),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e4caf77c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'isidentifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [127]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvalid_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\nangm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2658\u001b[0m, in \u001b[0;36mDatasetV2.unbatch\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;124;03m\"\"\"Splits elements of a dataset into multiple elements.\u001b[39;00m\n\u001b[0;32m   2635\u001b[0m \n\u001b[0;32m   2636\u001b[0m \u001b[38;5;124;03mFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2655\u001b[0m \u001b[38;5;124;03m  A `Dataset`.\u001b[39;00m\n\u001b[0;32m   2656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2657\u001b[0m normalized_dataset \u001b[38;5;241m=\u001b[39m normalize_to_dense(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 2658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_UnbatchDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\nangm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5920\u001b[0m, in \u001b[0;36m_UnbatchDataset.__init__\u001b[1;34m(self, input_dataset, name)\u001b[0m\n\u001b[0;32m   5918\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata \u001b[38;5;241m=\u001b[39m dataset_metadata_pb2\u001b[38;5;241m.\u001b[39mMetadata()\n\u001b[0;32m   5919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[1;32m-> 5920\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_and_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5921\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_structure\n\u001b[0;32m   5922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2021\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m30\u001b[39m):\n",
      "File \u001b[1;32mc:\\users\\nangm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:118\u001b[0m, in \u001b[0;36m_validate_and_encode\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_and_encode\u001b[39m(name):\n\u001b[1;32m--> 118\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misidentifier\u001b[49m():\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `name`. The argument `name` needs to be a valid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentifier. Value is considered a valid identifier if it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly contains alphanumeric characters (a-z), (A-Z), and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0-9), or underscores (_). A valid identifier cannot \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart with a number, or contain any spaces.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m name\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'isidentifier'"
     ]
    }
   ],
   "source": [
    "valid_set.unbatch(32).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ab18a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 256, 256, 32)\n",
      "IRB2 : batch_input_shape = (None, 256, 256, 16)\n",
      "IRB3 : batch_input_shape = (None, 128, 128, 16)\n",
      "IRB4 : batch_input_shape = (None, 64, 64, 32)\n",
      "IRB5 : batch_input_shape = (None, 32, 32, 64)\n",
      "IRB6 : batch_input_shape = (None, 16, 16, 128)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 512, 512, 3)      12        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 32)      896       \n",
      "                                                                 \n",
      " IRB1 (Inveted_Residual_Bloc  (None, 256, 256, 16)     2288      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB2 (Inveted_Residual_Bloc  (None, 128, 128, 16)     6688      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB3 (Inveted_Residual_Bloc  (None, 64, 64, 32)       19456     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB4 (Inveted_Residual_Bloc  (None, 32, 32, 64)       67584     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB5 (Inveted_Residual_Bloc  (None, 16, 16, 128)      108416    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB6 (Inveted_Residual_Bloc  (None, 16, 16, 1024)     1200640   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " re_lu_13 (ReLU)             (None, 16, 16, 1024)      0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                63550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,473,626\n",
      "Trainable params: 1,460,916\n",
      "Non-trainable params: 12,710\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #황성현\n",
    "        #multygpu\n",
    "    n_classes=62\n",
    "    smv2_l_adam=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[512,512,3]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=4,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=8,c=32,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=8,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=8,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_l_adam.summary()\n",
    "    smv2_l_adam.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7e3a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet_v2\n",
      "mobilenet_v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#황성현\n",
    "#csv_reader_dataset의 파라미터 model_name에 eModelNAme class의 맴버 변수 중 사용할 모델 입력\n",
    "#ex) model_name = eModelName.inception_v3, model_name = eModelName.renet \n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "batch_size=32\n",
    "train_set = csv_reader_dataset(train_filepaths, model_name = eModelName.mobilenet_v2, batch_size=batch_size, \n",
    "                               repeat=None,n_read_threads=tf.data.experimental.AUTOTUNE)\n",
    "valid_set = csv_reader_dataset(valid_filepaths, model_name = eModelName.mobilenet_v2, batch_size=batch_size, \n",
    "                               n_read_threads=tf.data.experimental.AUTOTUNE, repeat=None, randomize = False)\n",
    "train_set,valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3107bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현 \n",
    "from datetime import datetime\n",
    "logs = \"logs/\" + datetime.now().strftime(\"SMV2L_BATCH8%Y%m%d-%H%M%S\")\n",
    "\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2l_adam_batch8_best.h5\",save_best_only=True)\n",
    "check_last_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2l_adam_batch8_last.h5\",save_best_only=False)\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.001, s=10)\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history=smv2_l_adam.fit(train_set, batch_size = batch_size,\n",
    "                    steps_per_epoch=int(train_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(valid_size / batch_size),\n",
    "                    epochs=300,callbacks=[tboard_callback, check_best_cb, check_last_cb, earlystop_cb,lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6da58",
   "metadata": {},
   "source": [
    "# SMV2P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2d7974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 112, 112, 32)\n",
      "IRB2 : batch_input_shape = (None, 112, 112, 16)\n",
      "IRB3 : batch_input_shape = (None, 56, 56, 32)\n",
      "IRB4 : batch_input_shape = (None, 28, 28, 64)\n",
      "IRB5 : batch_input_shape = (None, 14, 14, 64)\n",
      "IRB6 : batch_input_shape = (None, 7, 7, 128)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_17 (Batc (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 112, 112, 32)      896       \n",
      "_________________________________________________________________\n",
      "IRB1 (Inveted_Residual_Block (None, 112, 112, 16)      2288      \n",
      "_________________________________________________________________\n",
      "IRB2 (Inveted_Residual_Block (None, 56, 56, 32)        9856      \n",
      "_________________________________________________________________\n",
      "IRB3 (Inveted_Residual_Block (None, 28, 28, 64)        67584     \n",
      "_________________________________________________________________\n",
      "IRB4 (Inveted_Residual_Block (None, 14, 14, 64)        151168    \n",
      "_________________________________________________________________\n",
      "IRB5 (Inveted_Residual_Block (None, 7, 7, 128)         108416    \n",
      "_________________________________________________________________\n",
      "IRB6 (Inveted_Residual_Block (None, 7, 7, 1024)        1200640   \n",
      "_________________________________________________________________\n",
      "re_lu_68 (ReLU)              (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_8 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 62)                63550     \n",
      "=================================================================\n",
      "Total params: 1,608,506\n",
      "Trainable params: 1,592,532\n",
      "Non-trainable params: 15,974\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#황성현\n",
    "with strategy.scope():\n",
    "    n_classes=62\n",
    "    smv2_p=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[224,224,3]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=4,c=32,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=8,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=8,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_p.summary()\n",
    "    smv2_p.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "930d75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "INFO:tensorflow:batch_all_reduce: 116 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 116 all-reduces with algorithm = nccl, num_packs = 1\n",
      "41552/41552 [==============================] - 4478s 107ms/step - loss: 0.3839 - accuracy: 0.8680 - val_loss: 5.9704 - val_accuracy: 0.3080\n",
      "Epoch 2/300\n",
      "41552/41552 [==============================] - 4422s 106ms/step - loss: 0.3190 - accuracy: 0.8827 - val_loss: 2.1717 - val_accuracy: 0.6591\n",
      "Epoch 3/300\n",
      "41552/41552 [==============================] - 4420s 106ms/step - loss: 0.2998 - accuracy: 0.8894 - val_loss: 2.7346 - val_accuracy: 0.6143\n",
      "Epoch 4/300\n",
      "41552/41552 [==============================] - 4411s 106ms/step - loss: 0.3061 - accuracy: 0.8864 - val_loss: 1.8020 - val_accuracy: 0.7005\n",
      "Epoch 5/300\n",
      "41552/41552 [==============================] - 4424s 106ms/step - loss: 0.3005 - accuracy: 0.8882 - val_loss: 2.5975 - val_accuracy: 0.6392\n",
      "Epoch 6/300\n",
      "41552/41552 [==============================] - 4401s 106ms/step - loss: 0.3003 - accuracy: 0.8884 - val_loss: 1.7739 - val_accuracy: 0.6988\n",
      "Epoch 7/300\n",
      "41552/41552 [==============================] - 4418s 106ms/step - loss: 0.3221 - accuracy: 0.8795 - val_loss: 2.3270 - val_accuracy: 0.6572\n",
      "Epoch 8/300\n",
      "41552/41552 [==============================] - 4421s 106ms/step - loss: 0.2990 - accuracy: 0.8886 - val_loss: 1.7881 - val_accuracy: 0.6934\n",
      "Epoch 9/300\n",
      "41552/41552 [==============================] - 4446s 107ms/step - loss: 0.3157 - accuracy: 0.8828 - val_loss: 1.3234 - val_accuracy: 0.7524\n",
      "Epoch 10/300\n",
      "41552/41552 [==============================] - 4461s 107ms/step - loss: 0.3381 - accuracy: 0.8744 - val_loss: 12.6891 - val_accuracy: 0.0540\n",
      "Epoch 11/300\n",
      "41552/41552 [==============================] - 4456s 107ms/step - loss: 0.3188 - accuracy: 0.8810 - val_loss: 1.6040 - val_accuracy: 0.7113\n",
      "Epoch 12/300\n",
      "41552/41552 [==============================] - 4473s 108ms/step - loss: 0.3073 - accuracy: 0.8862 - val_loss: 1.8713 - val_accuracy: 0.6818\n",
      "Epoch 13/300\n",
      "41552/41552 [==============================] - 4468s 108ms/step - loss: 3.5037 - accuracy: 0.1410 - val_loss: 60.9303 - val_accuracy: 0.0034\n",
      "Epoch 14/300\n",
      "41552/41552 [==============================] - 4444s 107ms/step - loss: 3.8621 - accuracy: 0.0588 - val_loss: 31.5947 - val_accuracy: 0.0026\n",
      "Epoch 15/300\n",
      "41552/41552 [==============================] - 4450s 107ms/step - loss: 3.8667 - accuracy: 0.0552 - val_loss: 48.2032 - val_accuracy: 0.0025\n",
      "Epoch 16/300\n",
      "41552/41552 [==============================] - 4386s 106ms/step - loss: 3.8738 - accuracy: 0.0592 - val_loss: 13.4360 - val_accuracy: 0.0089\n",
      "Epoch 17/300\n",
      "41552/41552 [==============================] - 4374s 105ms/step - loss: 3.8544 - accuracy: 0.0553 - val_loss: 10.2395 - val_accuracy: 0.0073\n",
      "Epoch 18/300\n",
      "41552/41552 [==============================] - 4362s 105ms/step - loss: 3.8699 - accuracy: 0.0570 - val_loss: 17.0552 - val_accuracy: 0.0073\n",
      "Epoch 19/300\n",
      "41552/41552 [==============================] - 4372s 105ms/step - loss: 3.8555 - accuracy: 0.0571 - val_loss: 21.8589 - val_accuracy: 0.0044\n"
     ]
    }
   ],
   "source": [
    "#황성현 \n",
    "from datetime import datetime\n",
    "logs = \"logs/\" + datetime.now().strftime(\"SMV2P%Y%m%d-%H%M%S\")\n",
    "print(logs)\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb = tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2p_nadam_best.h5\",save_best_only=True)\n",
    "check_last_cb = tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2p_nadam_last.h5\",save_best_only=False)\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "history=smv2_p.fit(train_set, batch_size = batch_size,\n",
    "                    steps_per_epoch=int(train_size / batch_size),\n",
    "                    validation_data = valid_set,\n",
    "                    validation_steps=int(valid_size / batch_size),\n",
    "                    epochs=300,callbacks=[tboard_callback, check_best_cb, check_last_cb, earlystop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "62195a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 112, 112, 32)\n",
      "IRB2 : batch_input_shape = (None, 112, 112, 16)\n",
      "IRB3 : batch_input_shape = (None, 56, 56, 32)\n",
      "IRB4 : batch_input_shape = (None, 28, 28, 64)\n",
      "IRB5 : batch_input_shape = (None, 14, 14, 64)\n",
      "IRB6 : batch_input_shape = (None, 7, 7, 128)\n"
     ]
    }
   ],
   "source": [
    "smv2_p=keras.models.load_model(\n",
    "    \"smv2p_nadam_best.h5\",custom_objects={\"Inveted_Residual_Block\":Inveted_Residual_Block})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "60262677",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8244d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17285), started 9:56:56 ago. (Use '!kill 17285' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ade5fa72bbfc5cb0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ade5fa72bbfc5cb0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/SMV2P20220525-203629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04f3db96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: Input 0 of layer sequential_8 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 692, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 382, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 463, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 1313, in run_step\n",
      "    outputs = model.test_step(data)\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 1267, in test_step\n",
      "    y_pred = self(x, training=False)\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 1020, in __call__\n",
      "    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n",
      "  File \"/home/dsd/.local/lib/python3.6/site-packages/keras/engine/input_spec.py\", line 218, in assert_input_compatibility\n",
      "    str(tuple(shape)))\n",
      "ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1330 test_function  *\n        return step_function(self, iterator)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py:671 _call_for_each_replica\n        self._container_strategy(), fn, args, kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica\n        return _call_for_each_replica(strategy, fn, args, kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_run.py:246 _call_for_each_replica\n        coord.join(threads)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    /home/dsd/.local/lib/python3.6/site-packages/six.py:703 reraise\n        raise value\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\n        yield\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_run.py:346 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1267 test_step\n        y_pred = self(x, training=False)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/input_spec.py:218 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-04e47490cfe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmv2_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3459\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3460\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3382\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1330 test_function  *\n        return step_function(self, iterator)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py:671 _call_for_each_replica\n        self._container_strategy(), fn, args, kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica\n        return _call_for_each_replica(strategy, fn, args, kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_run.py:246 _call_for_each_replica\n        coord.join(threads)\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    /home/dsd/.local/lib/python3.6/site-packages/six.py:703 reraise\n        raise value\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\n        yield\n    /home/dsd/.local/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_run.py:346 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/training.py:1267 test_step\n        y_pred = self(x, training=False)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/dsd/.local/lib/python3.6/site-packages/keras/engine/input_spec.py:218 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "smv2_p.evaluate(X_valid[np.where(y_valid>10)],y_valid[np.where(y_valid>10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c5c3e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "smv2p_pred=smv2_p.predict(valid_set,steps=int(valid_size/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef9a848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_GPU_THREAD_MODE\"]=\"gpu_private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea1c072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "IRB1 : batch_input_shape = (None, 112, 112, 32)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "IRB2 : batch_input_shape = (None, 112, 112, 16)\n",
      "IRB3 : batch_input_shape = (None, 56, 56, 32)\n",
      "IRB4 : batch_input_shape = (None, 28, 28, 64)\n",
      "IRB5 : batch_input_shape = (None, 14, 14, 64)\n",
      "IRB6 : batch_input_shape = (None, 7, 7, 128)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 112, 112, 32)      896       \n",
      "_________________________________________________________________\n",
      "IRB1 (Inveted_Residual_Block (None, 112, 112, 16)      2288      \n",
      "_________________________________________________________________\n",
      "IRB2 (Inveted_Residual_Block (None, 56, 56, 32)        9856      \n",
      "_________________________________________________________________\n",
      "IRB3 (Inveted_Residual_Block (None, 28, 28, 64)        67584     \n",
      "_________________________________________________________________\n",
      "IRB4 (Inveted_Residual_Block (None, 14, 14, 64)        151168    \n",
      "_________________________________________________________________\n",
      "IRB5 (Inveted_Residual_Block (None, 7, 7, 128)         108416    \n",
      "_________________________________________________________________\n",
      "IRB6 (Inveted_Residual_Block (None, 7, 7, 1024)        1200640   \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 62)                63550     \n",
      "=================================================================\n",
      "Total params: 1,608,506\n",
      "Trainable params: 1,592,532\n",
      "Non-trainable params: 15,974\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#황성현\n",
    "#multygpu\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    n_classes=62\n",
    "    smv2_p_optimize=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[224,224,3]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=4,c=32,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=8,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=8,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_p_optimize.summary()\n",
    "    smv2_p_optimize.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9117eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/SMV2P_OPTIMIZE20220526-210334\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 116 all-reduces with algorithm = nccl, num_packs = 1\n",
      "41552/41552 [==============================] - 4600s 110ms/step - loss: 0.3985 - accuracy: 0.8624 - val_loss: 2.2519 - val_accuracy: 0.6456\n",
      "Epoch 2/2\n",
      "41552/41552 [==============================] - 4623s 111ms/step - loss: 0.3085 - accuracy: 0.8865 - val_loss: 1.8596 - val_accuracy: 0.6720\n"
     ]
    }
   ],
   "source": [
    "#황성현 \n",
    "from datetime import datetime\n",
    "logs = \"logs/\" + datetime.now().strftime(\"SMV2P_OPTIMIZE%Y%m%d-%H%M%S\")\n",
    "print(logs)\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2p_opt_nadam_best.h5\",save_best_only=True)\n",
    "check_last_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2p_opt_nadam_last.h5\",save_best_only=False)\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "history2=smv2_p_optimize.fit(train_set, batch_size = batch_size,\n",
    "                    steps_per_epoch=int(train_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(valid_size / batch_size),\n",
    "                    epochs=2,callbacks=[tboard_callback, check_best_cb, check_last_cb, earlystop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6af8b",
   "metadata": {},
   "source": [
    "# X_train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d8fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 224, 224, 3), (1000,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape,y_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7c382a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final[np.where(y_train_raw<10)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "56163232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAHz0lEQVR4nO3dX2hX9R/H8e83Z468aM7+qVGr1YouZIQXUZchQXihFxIK3gRdBBF2FQRlMbrrqqC7wC6FBhFUgqVmiHdeCEoJlrhAVoiKQW7OrdtfsPM+9Z377bXt8bjsxeE7ps8O+OF8T3dubq4D5LlrqX8AYH7ihFDihFDihFDihFB9Lbt/yoXF153vP7pzQihxQihxQihxQihxQihxQihxQqi2c05WmZmZmSX77L4+fx3/lzsnhBInhBInhBInhBInhBInhBInhHKwtMzMzs6W+9TUVLlfvny53L/88svG7caNG+W1bTZu3Fjue/fubdwGBwcX9NnLkTsnhBInhBInhBInhBInhBInhHKUEubChQvl/uGHH5b7Dz/8UO5tRyl//fVXuS/EyMhIub/00kuNm6MUIIY4IZQ4IZQ4IZQ4IZQ4IZQ4IZRzziVQnSW+//775bVffPFFud+8ebPc+/v7y/3JJ58s98r169fLfWBgoNzXrl3b82evRO6cEEqcEEqcEEqcEEqcEEqcEEqcEMo55yJo+3rK8fHxxq3tHHN6errcH3/88XLft29fue/Zs6fcK2fPni33tnPMTZs29fzZK5E7J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4Tqzs3NVXs5Mr/Tp0+X++7duxu3ycnJ8tq33nqr3N94441yf+CBB8q92+2W+0K0Pe+5fv36xq2vb0Ufyc/7S3fnhFDihFDihFDihFDihFDihFAr+t+nF8vMzEy5Hzt2rNyr45KdO3eW17766qvl/uCDD5b7Yvr999/L/aOPPir36hjokUce6elnWs7cOSGUOCGUOCGUOCGUOCGUOCGUOCGUR8Z6cPz48XJ//fXXy/2XX35p3M6cOVNe+9RTT5X7QlWvJ6y+0rPTaT/HbPvK0K+++qpxW8irCZcBj4zBciJOCCVOCCVOCCVOCCVOCCVOCOV5zh4MDAyUe9tr+lrOlhdV22dXZ5kffPBBee1vv/1W7m2vH9ywYUO5rzbunBBKnBBKnBBKnBBKnBBKnBBKnBDKOWcPtmzZUu6jo6PlPjEx0bidPXu2vLbtucbbt2+X+7lz58q9OstsO8fctm1buY+NjZX7xo0by321ceeEUOKEUOKEUOKEUOKEUOKEUOKEUM45e9D23OELL7xQ7t9++23jdvDgwfLatmdJjx49Wu6fffZZuT/33HONW9v30m7fvr3c77nnnnLnn9w5IZQ4IZQ4IZQ4IZQ4IZQ4IZRXAC6Cttf4VV8Ref78+fLa++67r9yvXr1a7ps3by73b775pnF77LHHymvXrFlT7jTyCkBYTsQJocQJocQJocQJocQJocQJoTwytgieeOKJcn/xxRcbt7Yz0ravp7z//vvL/Z133in34eHhxq3bnfc4jkXizgmhxAmhxAmhxAmhxAmhxAmhxAmhnHP2oO01e23PZH7//feNW19f/UcyMzNT7gvlLDOHOyeEEieEEieEEieEEieEEieEEieE8r21PTh27Fi5f/zxx+V+5MiRxm337t3ltcePHy/3S5culfvWrVvL/dSpU41bf39/eS098721sJyIE0KJE0KJE0KJE0KJE0KJE0J5nnMeLWe/5TssO51O5/Dhw+X+8MMPN27vvfdeee3g4GC5f/LJJ+X+008/lfvPP//cuLWdkXoW9M5y54RQ4oRQ4oRQ4oRQ4oRQ4oRQq/KRsdnZ2XL/+uuvy/2VV14p902bNpX7oUOHGrdt27aV1/7666/l/u6775b7+Ph4uT/99NON2+eff15e23bUQiOPjMFyIk4IJU4IJU4IJU4IJU4IJU4ItSofGZuamir3EydOLOj60dHRcn/00UfLvTI0NFTu+/fvL/eTJ0+We/XI2HfffVde+8wzz5R72+sN+Sd3TgglTgglTgglTgglTgglTgglTgi1Kg+eJicny/3o0aPlvm7dunJ//vnny33Dhg3lXmn7+sl777233O++++5yn5mZadxu3LhRXsud5c4JocQJocQJocQJocQJocQJocQJoVblOeetW7fK/c8//yz36hV+nU6ns2PHjnL3XCP/hjsnhBInhBInhBInhBInhBInhPJv+vO46676/1m3b9/+P/0k//2zr127Vu7T09N38KdhMblzQihxQihxQihxQihxQihxQihxQqhVec7Z9sjXyy+/XO6ffvppuY+NjZX7m2++2bj9+OOP5bVtr/Br2//4449yHx4ebtyeffbZ8tq282H+G79NCCVOCCVOCCVOCCVOCCVOCCVOCLUqzznXrl1b7m3noNVr8jqdTmd8fLzcq7PIttcTTk1Nlfvs7Gy5r1+/vtwPHDjQuG3fvr281jnnneW3CaHECaHECaHECaHECaHECaHECaG6c3Nz1V6OK9XFixfL/bXXXiv3tmcyq++e7Xa75bVtrw8cGRkp9127dpX722+/3bj19/eX19Kzef/Q3TkhlDghlDghlDghlDghlDghlDghlHPOHly5cqXcJyYmyr367tihoaHy2rZnUR966KFyd1YZyTknLCfihFDihFDihFDihFDihFCOUpZA9Ttve2SMFclRCiwn4oRQ4oRQ4oRQ4oRQ4oRQ4oRQq/IVgEvNWSb/hjsnhBInhBInhBInhBInhBInhBInhGo753QgB0vEnRNCiRNCiRNCiRNCiRNCiRNC/Q1uRX9sAEmy0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20,30):\n",
    "    import cv2 as cv\n",
    "    %matplotlib inline\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    ids=80\n",
    "    X_new=X_train_raw[np.where(y_train_raw<10)][i]\n",
    "    y_new=y_train_final[np.where(y_train_raw<10)][i]\n",
    "    c=X_new\n",
    "\n",
    "\n",
    "    plt.imshow(c, cmap=\"binary\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff02639d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "ids=80\n",
    "X_new=X_train_raw[ids]\n",
    "y_new=y_train_raw[ids]\n",
    "c=X_new\n",
    "\n",
    "\n",
    "plt.imshow(c, cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "866c2e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 112, 112, 32)\n",
      "IRB2 : batch_input_shape = (None, 112, 112, 16)\n",
      "IRB3 : batch_input_shape = (None, 56, 56, 16)\n",
      "IRB4 : batch_input_shape = (None, 28, 28, 32)\n",
      "IRB5 : batch_input_shape = (None, 14, 14, 64)\n",
      "IRB6 : batch_input_shape = (None, 7, 7, 128)\n"
     ]
    }
   ],
   "source": [
    "smv2l_nadam_noaug=keras.models.load_model(\"smv2l_nadam_noaug_best.h5\",\n",
    "                                          custom_objects={\"Inveted_Residual_Block\":Inveted_Residual_Block})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "03b5a54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 10s 296ms/step - loss: 29.1229 - accuracy: 0.0110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[29.12285614013672, 0.010999999940395355]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smv2l_nadam_noaug.evaluate(X_train_final,y_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71079fe1",
   "metadata": {},
   "source": [
    "# SMV2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a00e2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 112, 112, 32)\n",
      "IRB2 : batch_input_shape = (None, 112, 112, 16)\n",
      "IRB3 : batch_input_shape = (None, 56, 56, 24)\n",
      "IRB4 : batch_input_shape = (None, 28, 28, 48)\n",
      "IRB5 : batch_input_shape = (None, 14, 14, 64)\n",
      "IRB6 : batch_input_shape = (None, 7, 7, 128)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_8 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 32)      896       \n",
      "_________________________________________________________________\n",
      "IRB1 (Inveted_Residual_Block (None, 112, 112, 16)      2288      \n",
      "_________________________________________________________________\n",
      "IRB2 (Inveted_Residual_Block (None, 56, 56, 24)        8272      \n",
      "_________________________________________________________________\n",
      "IRB3 (Inveted_Residual_Block (None, 28, 28, 48)        39936     \n",
      "_________________________________________________________________\n",
      "IRB4 (Inveted_Residual_Block (None, 14, 14, 64)        107328    \n",
      "_________________________________________________________________\n",
      "IRB5 (Inveted_Residual_Block (None, 7, 7, 128)         108416    \n",
      "_________________________________________________________________\n",
      "IRB6 (Inveted_Residual_Block (None, 7, 7, 1024)        1200640   \n",
      "_________________________________________________________________\n",
      "re_lu_34 (ReLU)              (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 62)                63550     \n",
      "=================================================================\n",
      "Total params: 1,535,434\n",
      "Trainable params: 1,521,092\n",
      "Non-trainable params: 14,342\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#황성현\n",
    "with strategy.scope():\n",
    "    n_classes=62\n",
    "    smv2_b=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[224,224,3]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=4,c=24,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=8,c=48,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=8,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=8,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_b.summary()\n",
    "    smv2_b.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cc16939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "logs = \"logs/\" + datetime.now().strftime(\"SMV2B%Y%m%d-%H%M%S\")\n",
    "# Clear out prior logging data.\n",
    "\n",
    "logdir = \"logs/image/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Define the basic TensorBoard callback.\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict()\n",
    "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "\n",
    "# Define the per-epoch callback.\n",
    "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d47df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "INFO:tensorflow:batch_all_reduce: 116 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 116 all-reduces with algorithm = nccl, num_packs = 1\n",
      " 4545/41552 [==>...........................] - ETA: 1:06:03 - loss: 0.7828 - accuracy: 0.7641"
     ]
    }
   ],
   "source": [
    "#황성현 \n",
    "\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2b_nadam_best.h5\",save_best_only=True)\n",
    "check_last_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2b_nadam_last.h5\",save_best_only=False)\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "\n",
    "history=smv2_b.fit(train_set, batch_size = batch_size,\n",
    "                    steps_per_epoch=int(train_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(valid_size / batch_size),\n",
    "                    epochs=300,callbacks=[tboard_callback, check_best_cb, check_last_cb, earlystop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b1a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ce84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
