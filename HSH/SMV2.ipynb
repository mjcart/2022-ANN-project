{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version:  1.0.2\n",
      "TF version:  2.7.0\n",
      "GPU installed:  True\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"TF version: \", tf.__version__)\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# GPU test\n",
    "print(\"GPU installed: \",tf.test.is_built_with_gpu_support())\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")    \n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "# You can find lists of datasets at https://www.tensorflow.org/datasets/catalog/overview\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "# Information of dataset split (train/validation/test)\n",
    "info.splits\n",
    "# Information of train dataset\n",
    "info.splits[\"train\"]\n",
    "# Name of each class\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes\n",
    "print(n_classes)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "# Split dataset using split method\n",
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)\n",
    "from functools import partial\n",
    "\n",
    "# Define preprocess functions\n",
    "def central_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop\n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
    "\n",
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
    "\n",
    "# Select central crop or randomized crop\n",
    "def preprocess(image, label, randomize=False):\n",
    "    if randomize:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = central_crop(image)\n",
    "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
    "    print(resized_image.shape)\n",
    "    final_image = keras.applications.mobilenet.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "# Define Train, validation and test datasets\n",
    "batch_size = 16\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "def preprocess(image,label ):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    resized_image2=tf.concat([resized_image,resized_image],axis=3)\n",
    "    resized_image2=tf.concat([resized_image2,resized_image],axis=3)\n",
    "    print(resized_image2.shape)\n",
    "    final_image = keras.applications.mobilenet.preprocess_input(resized_image2)\n",
    "    return final_image,label\n",
    "\n",
    "# Define Train, validation and test datasets\n",
    "batch_size = 16\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess)).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(55000, 28, 28, 1), dtype=tf.float64, name=None), TensorSpec(shape=(55000,), dtype=tf.uint8, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMV2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inveted_Residual_Block(keras.layers.Layer):\n",
    "    def __init__(self,c,s=1,t=1,n=1,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.layers.ReLU(max_value=6)\n",
    "        self.main_layers = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=c*t,kernel_size=1,strides=1,padding=\"SAME\",activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3,strides=1,padding=\"SAME\",activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=c,kernel_size=1,strides=1,padding=\"SAME\",activation=self.activation),\n",
    "            ]\n",
    "        self.strides_layers=[\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=c*t,kernel_size=1,strides=1,padding=\"SAME\",activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3,strides=s,padding=\"SAME\",activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=c,kernel_size=1,strides=1,padding=\"SAME\",activation=self.activation),\n",
    "        ]\n",
    "        self.s=s\n",
    "        self.t=t\n",
    "        self.c=c\n",
    "        self.n=n\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        \n",
    "        for n in range(0,self.n):\n",
    "            if n == 0:\n",
    "                for layer in self.strides_layers:\n",
    "                    Z = layer(Z)\n",
    "            else:\n",
    "                skip_Z = Z\n",
    "                for layer in self.main_layers:\n",
    "                    Z = layer(Z)\n",
    "                \n",
    "                Z += skip_Z\n",
    "    \n",
    "        return Z\n",
    "    \n",
    "    def get_config(self):\n",
    "        base=super().get_config()\n",
    "        return{**base,\"s\":self.s,\"t\":self.t,\"c\":self.c,\"n\":self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv2_l=keras.models.Sequential([\n",
    "    keras.layers.BatchNormalization(input_shape=[224,224,3]),\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "    Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "    Inveted_Residual_Block(t=4,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "    Inveted_Residual_Block(t=8,c=32,n=2,s=2,name=\"IRB3\"),\n",
    "    Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "    Inveted_Residual_Block(t=8,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "    Inveted_Residual_Block(t=8,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "    keras.layers.ReLU(max_value=6),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 112, 112, 32)      896       \n",
      "_________________________________________________________________\n",
      "IRB1 (Inveted_Residual_Block (None, 112, 112, 16)      1216      \n",
      "_________________________________________________________________\n",
      "IRB2 (Inveted_Residual_Block (None, 56, 56, 16)        6688      \n",
      "_________________________________________________________________\n",
      "IRB3 (Inveted_Residual_Block (None, 28, 28, 32)        38656     \n",
      "_________________________________________________________________\n",
      "IRB4 (Inveted_Residual_Block (None, 14, 14, 64)        134656    \n",
      "_________________________________________________________________\n",
      "IRB5 (Inveted_Residual_Block (None, 7, 7, 128)         216448    \n",
      "_________________________________________________________________\n",
      "IRB6 (Inveted_Residual_Block (None, 7, 7, 1024)        9594368   \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 10,002,161\n",
      "Trainable params: 9,955,723\n",
      "Non-trainable params: 46,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smv2_l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv2_l.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 1.5512 - accuracy: 0.3779 - val_loss: 1.7621 - val_accuracy: 0.1654\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 1.3904 - accuracy: 0.4415 - val_loss: 1.7141 - val_accuracy: 0.2390\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 16s 92ms/step - loss: 1.2982 - accuracy: 0.4964 - val_loss: 2.8134 - val_accuracy: 0.2390\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 1.2171 - accuracy: 0.5302 - val_loss: 1.2407 - val_accuracy: 0.5092\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 16s 96ms/step - loss: 1.1591 - accuracy: 0.5425 - val_loss: 1.2270 - val_accuracy: 0.5772\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 1.0863 - accuracy: 0.5879 - val_loss: 1.1859 - val_accuracy: 0.5423\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 16s 96ms/step - loss: 1.0530 - accuracy: 0.5981 - val_loss: 0.9143 - val_accuracy: 0.6654\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 0.9888 - accuracy: 0.6294 - val_loss: 1.0542 - val_accuracy: 0.6195\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 0.9336 - accuracy: 0.6526 - val_loss: 0.8107 - val_accuracy: 0.7132\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 16s 95ms/step - loss: 0.9826 - accuracy: 0.6366 - val_loss: 0.7902 - val_accuracy: 0.7261\n"
     ]
    }
   ],
   "source": [
    "check_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2l_nadam2.h5\",save_best_only=True)\n",
    "history=smv2_l.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=10,callbacks=[check_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMV2P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv2_p=keras.models.Sequential([\n",
    "    keras.layers.BatchNormalization(input_shape=[224,224,3]),\n",
    "    keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "    Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "    Inveted_Residual_Block(t=4,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "    Inveted_Residual_Block(t=8,c=64,n=2,s=2,name=\"IRB3\"),\n",
    "    Inveted_Residual_Block(t=8,c=128,n=2,s=2,name=\"IRB4\"),\n",
    "    Inveted_Residual_Block(t=8,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "    Inveted_Residual_Block(t=8,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "    keras.layers.ReLU(max_value=6),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 224, 224, 3)      12        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 112, 112, 32)      896       \n",
      "                                                                 \n",
      " IRB1 (Inveted_Residual_Bloc  (None, 112, 112, 16)     1216      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB2 (Inveted_Residual_Bloc  (None, 56, 56, 16)       6688      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB3 (Inveted_Residual_Bloc  (None, 28, 28, 64)       126400    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB4 (Inveted_Residual_Bloc  (None, 14, 14, 128)      498688    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB5 (Inveted_Residual_Bloc  (None, 7, 7, 128)        282240    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB6 (Inveted_Residual_Bloc  (None, 7, 7, 1024)       9594368   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,524,854\n",
      "Trainable params: 10,471,888\n",
      "Non-trainable params: 52,966\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smv2_p.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv2_p.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 55000, 224, 224, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0acc077ac2fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcheck_cb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"smv2p_nadam.h5\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history=smv2_p.fit(train_set,\n\u001b[0m\u001b[0;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     epochs=5)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 55000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "check_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2p_nadam.h5\",save_best_only=True)\n",
    "history=smv2_p.fit(train_set,\n",
    "                    steps_per_epoch=int( dataset_size / batch_size),\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4342 - accuracy: 0.2918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4341720938682556, 0.29178759455680847]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=keras.models.load_model(\"smv2l_nadam.h5\",custom_objects={\"Inveted_Residual_Block\" : Inveted_Residual_Block})\n",
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 23ms/step - loss: 1.1099 - accuracy: 0.3706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1099331378936768, 0.37057220935821533]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=keras.models.load_model(\"my_mobilenetv2_1.h5\",\n",
    "                               custom_objects={\"Inveted_Residual_Block\" : Inveted_Residual_Block})\n",
    "model2.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5485 - accuracy: 0.2779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5485105514526367, 0.2779291570186615]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3=keras.models.load_model(\"smv2p_nadam.h5\",\n",
    "                               custom_objects={\"Inveted_Residual_Block\" : Inveted_Residual_Block})\n",
    "model3.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 19ms/step - loss: 0.4999 - accuracy: 0.2585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4998670816421509, 0.25853991508483887]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 24ms/step - loss: 0.5297 - accuracy: 0.2468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5296900868415833, 0.24682395160198212]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\hgh11\\tensorflow_datasets\\emnist\\byclass\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be52b0744eb49e0873008ac5c2b1524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fa01c186d4406eafdd04d69a347804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c546d35daf42d3a7d21b569cbfc4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset, info = tfds.load(\"emnist\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
