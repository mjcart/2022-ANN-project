{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9289fccd-b496-4478-ab47-16d5f0112be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version:  1.0.2\n",
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow_addons) (21.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.9/site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow_addons) (3.0.3)\n",
      "TF version:  2.8.0\n",
      "GPU installed:  True\n",
      "4 Physical GPUs, 4 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 05:35:39.470470: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 05:35:41.997031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14791 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:19:00.0, compute capability: 7.5\n",
      "2022-06-09 05:35:41.998036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14791 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2022-06-09 05:35:41.998983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 3276 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2022-06-09 05:35:41.999664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 3135 MB memory:  -> device: 3, name: Quadro RTX 5000, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "!pip install tensorflow_addons\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "print(\"TF version: \", tf.__version__)\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# GPU test\n",
    "print(\"GPU installed: \",tf.test.is_built_with_gpu_support())\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")    \n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43846f0-ee03-4e04-8da2-36dcdcde6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.uint8(np.load('./X_train_aug.npy'))\n",
    "y_train=np.uint8(np.load('./y_train_aug.npy'))\n",
    "X_valid=np.uint8(np.load('./X_valid.npy'))\n",
    "y_valid=np.uint8(np.load('./y_valid.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2514ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1074263, 32, 32, 1), (164015, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4228f8-9224-42bf-80fa-d905375844fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 05:35:49.767211: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1074263, 32, 32, 1), (164015, 32, 32, 1), tf.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid=tf.reshape(X_valid,[-1,28,28,1])\n",
    "X_train=tf.reshape(X_train,[-1,32,32,1])\n",
    "X_valid_resize=np.uint8(tf.image.resize(X_valid, [32, 32]))\n",
    "X_train_resize=np.uint8(tf.image.resize(X_train, [32, 32]))\n",
    "X_train_resize.shape,X_valid_resize.shape,X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58f9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=keras.applications.mobilenet_v2.preprocess_input(np.array(X_train,np.float32))\n",
    "X_valid_final=keras.applications.mobilenet_v2.preprocess_input(np.array(X_valid_resize,np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3071a1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa6eda-4eba-4e2b-8f42-83f61993a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현 전민재 He_normal initializer\n",
    "class Inveted_Residual_Block(keras.layers.Layer):\n",
    "    def __init__(self,c,s=1,t=1,n=1,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.layers.ReLU(max_value=6)\n",
    "        self.main_layers = [\n",
    "            \n",
    "            ]\n",
    "        self.strides_layers=[\n",
    "           ]\n",
    "        self.s=s\n",
    "        self.t=t\n",
    "        self.c=c\n",
    "        self.n=n\n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        print(self.name,\": batch_input_shape =\",batch_input_shape)\n",
    "        initializer = tf.keras.initializers.HeNormal()\n",
    "        self.main_layers = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=batch_input_shape[-1]*self.t, kernel_size=1,strides=1, kernel_initializer=initializer,\n",
    "                                padding=\"SAME\", activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3, strides=1,padding=\"SAME\", activation=self.activation,kernel_initializer=initializer),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=self.c,kernel_size=1, strides=1,padding=\"SAME\", activation=self.activation,kernel_initializer=initializer),\n",
    "            ]\n",
    "        self.strides_layers=[\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=batch_input_shape[-1]*self.t, kernel_size=1,strides=1,kernel_initializer=initializer,\n",
    "                                padding=\"SAME\", activation=self.activation),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3, strides=self.s,padding=\"SAME\",activation=self.activation,kernel_initializer=initializer),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=self.c, kernel_size=1,strides=1,padding=\"SAME\",activation=self.activation,kernel_initializer=initializer),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        \n",
    "        for n in range(0,self.n):\n",
    "            if n == 0:\n",
    "                for layer in self.strides_layers:\n",
    "                    Z = layer(Z)\n",
    "            else:\n",
    "                skip_Z = Z\n",
    "                for layer in self.main_layers:\n",
    "                    Z = layer(Z)\n",
    "                \n",
    "                Z += skip_Z\n",
    "    \n",
    "        return Z\n",
    "    \n",
    "    def get_config(self):\n",
    "        base=super().get_config()\n",
    "        return{**base,\"s\":self.s,\"t\":self.t,\"c\":self.c,\"n\":self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2261b9-a55e-4ba4-a8be-c9f8b86b3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#황성현 \n",
    "class Inveted_Residual_Block(keras.layers.Layer):\n",
    "    def __init__(self,c,s=1,t=1,n=1,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.layers.ReLU(max_value=6)\n",
    "        self.main_layers = [\n",
    "            \n",
    "            ]\n",
    "        self.strides_layers=[\n",
    "           ]\n",
    "        self.s=s\n",
    "        self.t=t\n",
    "        self.c=c\n",
    "        self.n=n\n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        print(self.name,\": batch_input_shape =\",batch_input_shape)\n",
    "        initializer = tf.keras.initializers.HeNormal()\n",
    "        self.main_layers = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=batch_input_shape[-1]*self.t, kernel_size=1,strides=1, kernel_initializer=initializer,\n",
    "                                padding=\"SAME\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3, strides=1,padding=\"SAME\",kernel_initializer=initializer),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.Conv2D(filters=self.c,kernel_size=1, strides=1,padding=\"SAME\",kernel_initializer=initializer),\n",
    "            ]\n",
    "        self.strides_layers=[\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv2D(filters=batch_input_shape[-1]*self.t, kernel_size=1,strides=1,kernel_initializer=initializer,\n",
    "                                padding=\"SAME\"),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.DepthwiseConv2D(kernel_size=3, strides=self.s,padding=\"SAME\",kernel_initializer=initializer),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.Conv2D(filters=self.c, kernel_size=1,strides=1,padding=\"SAME\",kernel_initializer=initializer),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        \n",
    "        for n in range(0,self.n):\n",
    "            if n == 0:\n",
    "                for layer in self.strides_layers:\n",
    "                    Z = layer(Z)\n",
    "            else:\n",
    "                skip_Z = Z\n",
    "                for layer in self.main_layers:\n",
    "                    Z = layer(Z)\n",
    "                \n",
    "                Z += skip_Z\n",
    "    \n",
    "        return Z\n",
    "    \n",
    "    def get_config(self):\n",
    "        base=super().get_config()\n",
    "        return{**base,\"s\":self.s,\"t\":self.t,\"c\":self.c,\"n\":self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34fb2330-2ff3-4979-9901-e2f0da8d336a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 16, 16, 32)\n",
      "IRB2 : batch_input_shape = (None, 16, 16, 16)\n",
      "IRB3 : batch_input_shape = (None, 8, 8, 16)\n",
      "IRB4 : batch_input_shape = (None, 4, 4, 32)\n",
      "IRB5 : batch_input_shape = (None, 2, 2, 64)\n",
      "IRB6 : batch_input_shape = (None, 1, 1, 128)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 32, 32, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " IRB1 (Inveted_Residual_Bloc  (None, 16, 16, 16)       2288      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB2 (Inveted_Residual_Bloc  (None, 8, 8, 16)         3424      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB3 (Inveted_Residual_Bloc  (None, 4, 4, 32)         9856      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB4 (Inveted_Residual_Bloc  (None, 2, 2, 64)         34048     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB5 (Inveted_Residual_Bloc  (None, 1, 1, 128)        54400     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB6 (Inveted_Residual_Bloc  (None, 1, 1, 512)        338432    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 62)                31806     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 476,626\n",
      "Trainable params: 469,808\n",
      "Non-trainable params: 6,818\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #황성현 전민재 IRB6512\n",
    "    #multygpu\n",
    "\n",
    "\n",
    "    n_classes=62\n",
    "    smv2_l=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[32,32,1]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=2,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=4,c=32,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=4,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=4,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=4,c=512,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_l.summary()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    batch_size=128\n",
    "    smv2_l.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b384ece7-e314-4e22-bc57-4bc9b780f8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#전민재\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)\n",
    "    def on_epoch_end(self, batch, logs):\n",
    "        print(\"lr: {}\".format(self.model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b42dfc-42dd-4800-8650-d76203880b03",
   "metadata": {},
   "source": [
    "smv2sl_adam_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch40\n",
    "lr =0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acf0157-3b8b-4112-bc4a-1b09e49035bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 09:29:00.404297: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-08 09:29:00.404350: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-06-08 09:29:00.404420: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 4 GPUs\n",
      "2022-06-08 09:29:00.405233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-06-08 09:29:01.179426: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-08 09:29:01.179648: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-08 09:29:01.271363: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n",
      "2022-06-08 09:29:04.097477: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 09:29:08.696503: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 497/8393 [>.............................] - ETA: 3:02 - loss: 4.9271 - accuracy: 0.0420"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 09:29:21.608695: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-08 09:29:21.608737: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 518/8393 [>.............................] - ETA: 3:16 - loss: 4.9033 - accuracy: 0.0433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 09:29:23.102770: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-06-08 09:29:23.104029: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-08 09:29:23.293156: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 14785 callback api events and 14858 activity events. \n",
      "2022-06-08 09:29:23.537231: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-08 09:29:23.855002: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23\n",
      "\n",
      "2022-06-08 09:29:24.084162: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23/a58a2589147d.trace.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 525/8393 [>.............................] - ETA: 3:36 - loss: 4.8973 - accuracy: 0.0436"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 09:29:24.357003: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23\n",
      "\n",
      "2022-06-08 09:29:24.364500: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23/a58a2589147d.memory_profile.json.gz\n",
      "2022-06-08 09:29:24.370768: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23\n",
      "Dumped tool data for xplane.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23/a58a2589147d.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23/a58a2589147d.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23/a58a2589147d.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23/a58a2589147d.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch4020220608-092900/plugins/profile/2022_06_08_09_29_23/a58a2589147d.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8393/8393 [==============================] - ETA: 0s - loss: 1.9272 - accuracy: 0.4866lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00030000065>\n",
      "8393/8393 [==============================] - 215s 25ms/step - loss: 1.9272 - accuracy: 0.4866 - val_loss: 0.5558 - val_accuracy: 0.8033\n",
      "Epoch 2/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.5492 - accuracy: 0.8078lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00040001323>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.5492 - accuracy: 0.8078 - val_loss: 0.4254 - val_accuracy: 0.8430\n",
      "Epoch 3/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.8335lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0005000258>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.4605 - accuracy: 0.8335 - val_loss: 0.4594 - val_accuracy: 0.8366\n",
      "Epoch 4/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.4296 - accuracy: 0.8430lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0006000384>\n",
      "8393/8393 [==============================] - 209s 25ms/step - loss: 0.4296 - accuracy: 0.8430 - val_loss: 0.4099 - val_accuracy: 0.8467\n",
      "Epoch 5/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.8475lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00070005096>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.4133 - accuracy: 0.8475 - val_loss: 0.4056 - val_accuracy: 0.8512\n",
      "Epoch 6/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.4037 - accuracy: 0.8508lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00080006354>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4037 - accuracy: 0.8508 - val_loss: 0.4960 - val_accuracy: 0.8200\n",
      "Epoch 7/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8531lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009000761>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3964 - accuracy: 0.8531 - val_loss: 0.7728 - val_accuracy: 0.7697\n",
      "Epoch 8/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3909 - accuracy: 0.8549lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0010000888>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3909 - accuracy: 0.8549 - val_loss: 0.5032 - val_accuracy: 0.8178\n",
      "Epoch 9/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3863 - accuracy: 0.8562lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0011001013>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3863 - accuracy: 0.8562 - val_loss: 1.1246 - val_accuracy: 0.6581\n",
      "Epoch 10/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.8572lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0012001139>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3831 - accuracy: 0.8572 - val_loss: 0.5526 - val_accuracy: 0.8089\n",
      "Epoch 11/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.8580lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0013001264>\n",
      "8393/8393 [==============================] - 209s 25ms/step - loss: 0.3805 - accuracy: 0.8580 - val_loss: 0.5287 - val_accuracy: 0.8150\n",
      "Epoch 12/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.8586lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0014001391>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3785 - accuracy: 0.8586 - val_loss: 0.3686 - val_accuracy: 0.8628\n",
      "Epoch 13/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8595lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0015001516>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3761 - accuracy: 0.8595 - val_loss: 0.6825 - val_accuracy: 0.7925\n",
      "Epoch 14/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8597lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016001642>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3742 - accuracy: 0.8597 - val_loss: 0.4176 - val_accuracy: 0.8454\n",
      "Epoch 15/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3737 - accuracy: 0.8601lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0017001767>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3737 - accuracy: 0.8601 - val_loss: 0.4375 - val_accuracy: 0.8491\n",
      "Epoch 16/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3721 - accuracy: 0.8605lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018001894>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.3721 - accuracy: 0.8605 - val_loss: 0.8949 - val_accuracy: 0.7658\n",
      "Epoch 17/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3721 - accuracy: 0.8607lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0019002019>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3721 - accuracy: 0.8607 - val_loss: 0.9358 - val_accuracy: 0.7538\n",
      "Epoch 18/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3730 - accuracy: 0.8605lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0019997854>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3730 - accuracy: 0.8604 - val_loss: 0.3778 - val_accuracy: 0.8619\n",
      "Epoch 19/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8614lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018997729>\n",
      "8393/8393 [==============================] - 209s 25ms/step - loss: 0.3696 - accuracy: 0.8614 - val_loss: 0.3772 - val_accuracy: 0.8603\n",
      "Epoch 20/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8630lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0017997604>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3637 - accuracy: 0.8630 - val_loss: 0.3762 - val_accuracy: 0.8625\n",
      "Epoch 21/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8650lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016997478>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3577 - accuracy: 0.8650 - val_loss: 0.8648 - val_accuracy: 0.7531\n",
      "Epoch 22/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.8665lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0015997352>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3520 - accuracy: 0.8665 - val_loss: 0.3622 - val_accuracy: 0.8660\n",
      "Epoch 23/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.8681lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0014997226>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3468 - accuracy: 0.8681 - val_loss: 0.4319 - val_accuracy: 0.8484\n",
      "Epoch 24/40\n",
      "8390/8393 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8693lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0013997101>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3418 - accuracy: 0.8693 - val_loss: 0.3490 - val_accuracy: 0.8707\n",
      "Epoch 25/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8709lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0012996974>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3371 - accuracy: 0.8709 - val_loss: 0.3503 - val_accuracy: 0.8695\n",
      "Epoch 26/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8729lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0011996849>\n",
      "8393/8393 [==============================] - 210s 25ms/step - loss: 0.3315 - accuracy: 0.8729 - val_loss: 0.4981 - val_accuracy: 0.8391\n",
      "Epoch 27/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.8738lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0010996723>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3266 - accuracy: 0.8738 - val_loss: 0.6815 - val_accuracy: 0.7855\n",
      "Epoch 28/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8758lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009996598>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3216 - accuracy: 0.8758 - val_loss: 0.3411 - val_accuracy: 0.8732\n",
      "Epoch 29/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8771lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00089964713>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3171 - accuracy: 0.8771 - val_loss: 0.4089 - val_accuracy: 0.8602\n",
      "Epoch 30/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3116 - accuracy: 0.8787lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00079963455>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3116 - accuracy: 0.8787 - val_loss: 0.3389 - val_accuracy: 0.8749\n",
      "Epoch 31/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8802lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.000699622>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3069 - accuracy: 0.8802 - val_loss: 0.4638 - val_accuracy: 0.8469\n",
      "Epoch 32/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8818lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0005996094>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3019 - accuracy: 0.8818 - val_loss: 1.3481 - val_accuracy: 0.7233\n",
      "Epoch 33/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8833lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0004995968>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.2971 - accuracy: 0.8833 - val_loss: 0.3332 - val_accuracy: 0.8764\n",
      "Epoch 34/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8848lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00039958427>\n",
      "8393/8393 [==============================] - 210s 25ms/step - loss: 0.2922 - accuracy: 0.8848 - val_loss: 0.3364 - val_accuracy: 0.8757\n",
      "Epoch 35/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.8863lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0002995717>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.2876 - accuracy: 0.8863 - val_loss: 0.3345 - val_accuracy: 0.8769\n",
      "Epoch 36/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.8879lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00019977978>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.2827 - accuracy: 0.8879 - val_loss: 0.4645 - val_accuracy: 0.8553\n",
      "Epoch 37/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8891lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00014982681>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.2790 - accuracy: 0.8891 - val_loss: 0.3353 - val_accuracy: 0.8778\n",
      "Epoch 38/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.8901lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=9.987383e-05>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.2765 - accuracy: 0.8901 - val_loss: 0.3354 - val_accuracy: 0.8783\n",
      "Epoch 39/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8909lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=4.9920858e-05>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.2738 - accuracy: 0.8909 - val_loss: 0.3368 - val_accuracy: 0.8781\n",
      "Epoch 40/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.2718 - accuracy: 0.8915lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=2e-07>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.2718 - accuracy: 0.8915 - val_loss: 0.3362 - val_accuracy: 0.8783\n"
     ]
    }
   ],
   "source": [
    "#황성현 전민재\n",
    "from datetime import datetime\n",
    "K = keras.backend\n",
    "epochs=40\n",
    "logs = \"logs/\" + datetime.now().strftime(f\"smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch{epochs}%Y%m%d-%H%M%S\")\n",
    "print(logs)\n",
    "onecycle = OneCycleScheduler(len(X_train_resize) // batch_size * epochs, max_rate=0.002)\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=f\"smv2l_adam_best_lrsch002_onecycle_aug_dropout_128_he_irb6_512_t4_epoch{epochs}.h5\",save_best_only=True)\n",
    "\n",
    "history=smv2_l.fit(X_train,y_train,\n",
    "                   validation_data=(X_valid_final,y_valid),batch_size = batch_size, \n",
    "                   epochs=epochs,callbacks=[tboard_callback, check_best_cb,onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f04d2-6275-45aa-8bfe-d7cffdad146a",
   "metadata": {},
   "source": [
    "maxlr 0.003 dropout 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d72222-6a4b-4987-bcbd-0466f3fb7c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 16, 16, 32)\n",
      "IRB2 : batch_input_shape = (None, 16, 16, 16)\n",
      "IRB3 : batch_input_shape = (None, 8, 8, 16)\n",
      "IRB4 : batch_input_shape = (None, 4, 4, 32)\n",
      "IRB5 : batch_input_shape = (None, 2, 2, 64)\n",
      "IRB6 : batch_input_shape = (None, 1, 1, 128)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 32, 32, 1)        4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " IRB1 (Inveted_Residual_Bloc  (None, 16, 16, 16)       2288      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB2 (Inveted_Residual_Bloc  (None, 8, 8, 16)         3424      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB3 (Inveted_Residual_Bloc  (None, 4, 4, 32)         9856      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB4 (Inveted_Residual_Bloc  (None, 2, 2, 64)         34048     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB5 (Inveted_Residual_Bloc  (None, 1, 1, 128)        54400     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB6 (Inveted_Residual_Bloc  (None, 1, 1, 512)        338432    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " re_lu_13 (ReLU)             (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                31806     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 476,626\n",
      "Trainable params: 469,808\n",
      "Non-trainable params: 6,818\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #황성현 전민재 IRB6512, maxlr 0.003 dropout 0.7\n",
    "    #multygpu\n",
    "\n",
    "\n",
    "    n_classes=62\n",
    "    smv2_l=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[32,32,1]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=2,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=4,c=32,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=4,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=4,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=4,c=512,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.7),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_l.summary()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    batch_size=128\n",
    "    smv2_l.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24362c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 13:35:58.854450: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-08 13:35:58.854486: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-06-08 13:35:58.854531: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 4 GPUs\n",
      "2022-06-08 13:35:58.855298: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-06-08 13:35:59.659079: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-08 13:35:59.659340: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-08 13:35:59.747827: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n",
      "2022-06-08 13:36:02.705083: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 13:36:07.137433: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 499/8393 [>.............................] - ETA: 3:01 - loss: 5.5227 - accuracy: 0.0342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 13:36:20.102666: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-08 13:36:20.102724: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 518/8393 [>.............................] - ETA: 3:16 - loss: 5.4961 - accuracy: 0.0349"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 13:36:21.614603: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-06-08 13:36:21.615946: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-08 13:36:21.801217: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 14722 callback api events and 14795 activity events. \n",
      "2022-06-08 13:36:22.073949: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-08 13:36:22.386559: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22\n",
      "\n",
      "2022-06-08 13:36:22.615295: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22/a58a2589147d.trace.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 525/8393 [>.............................] - ETA: 3:36 - loss: 5.4865 - accuracy: 0.0352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 13:36:22.876807: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22\n",
      "\n",
      "2022-06-08 13:36:22.883988: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22/a58a2589147d.memory_profile.json.gz\n",
      "2022-06-08 13:36:22.889210: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22\n",
      "Dumped tool data for xplane.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22/a58a2589147d.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22/a58a2589147d.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22/a58a2589147d.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22/a58a2589147d.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch4020220608-133558/plugins/profile/2022_06_08_13_36_22/a58a2589147d.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8393/8393 [==============================] - ETA: 0s - loss: 1.9257 - accuracy: 0.5047lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00045000098>\n",
      "8393/8393 [==============================] - 214s 25ms/step - loss: 1.9257 - accuracy: 0.5047 - val_loss: 0.5293 - val_accuracy: 0.8153\n",
      "Epoch 2/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.5492 - accuracy: 0.8102lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0006000199>\n",
      "8393/8393 [==============================] - 216s 26ms/step - loss: 0.5492 - accuracy: 0.8102 - val_loss: 0.4349 - val_accuracy: 0.8416\n",
      "Epoch 3/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.8327lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0007500387>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4715 - accuracy: 0.8327 - val_loss: 0.4145 - val_accuracy: 0.8477\n",
      "Epoch 4/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.8411lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009000576>\n",
      "8393/8393 [==============================] - 206s 24ms/step - loss: 0.4435 - accuracy: 0.8411 - val_loss: 0.4285 - val_accuracy: 0.8424\n",
      "Epoch 5/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.4287 - accuracy: 0.8451lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0010500764>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4287 - accuracy: 0.8451 - val_loss: 0.4331 - val_accuracy: 0.8379\n",
      "Epoch 6/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.8481lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0012000953>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.4198 - accuracy: 0.8481 - val_loss: 0.3875 - val_accuracy: 0.8585\n",
      "Epoch 7/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.4136 - accuracy: 0.8501lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0013501142>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.4136 - accuracy: 0.8501 - val_loss: 0.7183 - val_accuracy: 0.7874\n",
      "Epoch 8/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.4098 - accuracy: 0.8512lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0015001331>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.4099 - accuracy: 0.8511 - val_loss: 0.3754 - val_accuracy: 0.8616\n",
      "Epoch 9/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8517lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016501519>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4064 - accuracy: 0.8517 - val_loss: 0.3838 - val_accuracy: 0.8586\n",
      "Epoch 10/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8527lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018001708>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4036 - accuracy: 0.8527 - val_loss: 0.3995 - val_accuracy: 0.8559\n",
      "Epoch 11/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4017 - accuracy: 0.8535lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0019501897>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4017 - accuracy: 0.8535 - val_loss: 0.3861 - val_accuracy: 0.8577\n",
      "Epoch 12/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4008 - accuracy: 0.8538lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0021002085>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4008 - accuracy: 0.8538 - val_loss: 0.3865 - val_accuracy: 0.8585\n",
      "Epoch 13/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8542lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022502274>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3997 - accuracy: 0.8542 - val_loss: 0.4377 - val_accuracy: 0.8430\n",
      "Epoch 14/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8541lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0024002462>\n",
      "8393/8393 [==============================] - 209s 25ms/step - loss: 0.4000 - accuracy: 0.8541 - val_loss: 0.5169 - val_accuracy: 0.8241\n",
      "Epoch 15/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.8540lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002550265>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4002 - accuracy: 0.8540 - val_loss: 0.4939 - val_accuracy: 0.8315\n",
      "Epoch 16/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.8543lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002700284>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4005 - accuracy: 0.8543 - val_loss: 0.5600 - val_accuracy: 0.8195\n",
      "Epoch 17/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.8536lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002850303>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4020 - accuracy: 0.8536 - val_loss: 0.3889 - val_accuracy: 0.8601\n",
      "Epoch 18/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.4046 - accuracy: 0.8532lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0029996783>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4046 - accuracy: 0.8532 - val_loss: 1.4306 - val_accuracy: 0.6365\n",
      "Epoch 19/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8538lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028496594>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.4041 - accuracy: 0.8538 - val_loss: 0.3821 - val_accuracy: 0.8591\n",
      "Epoch 20/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3975 - accuracy: 0.8555lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0026996406>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3975 - accuracy: 0.8555 - val_loss: 0.3921 - val_accuracy: 0.8583\n",
      "Epoch 21/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3912 - accuracy: 0.8571lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0025496217>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3912 - accuracy: 0.8571 - val_loss: 0.4622 - val_accuracy: 0.8350\n",
      "Epoch 22/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.8591lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002399603>\n",
      "8393/8393 [==============================] - 209s 25ms/step - loss: 0.3851 - accuracy: 0.8591 - val_loss: 0.4161 - val_accuracy: 0.8451\n",
      "Epoch 23/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8603lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022495838>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3791 - accuracy: 0.8603 - val_loss: 0.3783 - val_accuracy: 0.8623\n",
      "Epoch 24/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8625lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002099565>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3728 - accuracy: 0.8625 - val_loss: 0.3906 - val_accuracy: 0.8603\n",
      "Epoch 25/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3669 - accuracy: 0.8640lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0019495462>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3669 - accuracy: 0.8640 - val_loss: 0.3734 - val_accuracy: 0.8645\n",
      "Epoch 26/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3613 - accuracy: 0.8656lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0017995273>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3613 - accuracy: 0.8656 - val_loss: 0.4250 - val_accuracy: 0.8503\n",
      "Epoch 27/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.8670lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016495085>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3554 - accuracy: 0.8670 - val_loss: 0.4528 - val_accuracy: 0.8506\n",
      "Epoch 28/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3495 - accuracy: 0.8688lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0014994895>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3495 - accuracy: 0.8688 - val_loss: 0.3583 - val_accuracy: 0.8672\n",
      "Epoch 29/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.8707lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0013494707>\n",
      "8393/8393 [==============================] - 209s 25ms/step - loss: 0.3434 - accuracy: 0.8707 - val_loss: 0.3592 - val_accuracy: 0.8671\n",
      "Epoch 30/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.8721lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0011994519>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3382 - accuracy: 0.8721 - val_loss: 0.3432 - val_accuracy: 0.8736\n",
      "Epoch 31/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.8736lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001049433>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3319 - accuracy: 0.8736 - val_loss: 0.3482 - val_accuracy: 0.8715\n",
      "Epoch 32/40\n",
      "8390/8393 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8757lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008994141>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3261 - accuracy: 0.8757 - val_loss: 0.3423 - val_accuracy: 0.8738\n",
      "Epoch 33/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.8771lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0007493952>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3204 - accuracy: 0.8771 - val_loss: 0.3393 - val_accuracy: 0.8732\n",
      "Epoch 34/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.8791lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0005993764>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3146 - accuracy: 0.8791 - val_loss: 0.3367 - val_accuracy: 0.8752\n",
      "Epoch 35/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8811lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00044935753>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3089 - accuracy: 0.8811 - val_loss: 0.3321 - val_accuracy: 0.8774\n",
      "Epoch 36/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8826lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0002996697>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3030 - accuracy: 0.8826 - val_loss: 0.3356 - val_accuracy: 0.8757\n",
      "Epoch 37/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.8844lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00022474022>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.2987 - accuracy: 0.8844 - val_loss: 0.3328 - val_accuracy: 0.8779\n",
      "Epoch 38/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8853lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00014981074>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.2957 - accuracy: 0.8853 - val_loss: 0.3321 - val_accuracy: 0.8780\n",
      "Epoch 39/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8860lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.488129e-05>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.2925 - accuracy: 0.8860 - val_loss: 0.3318 - val_accuracy: 0.8784\n",
      "Epoch 40/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8868lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=3e-07>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.2898 - accuracy: 0.8868 - val_loss: 0.3318 - val_accuracy: 0.8784\n"
     ]
    }
   ],
   "source": [
    "#황성현 전민재\n",
    "from datetime import datetime\n",
    "K = keras.backend\n",
    "epochs=40\n",
    "logs = \"logs/\" + datetime.now().strftime(f\"smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch{epochs}_noact%Y%m%d-%H%M%S\")\n",
    "print(logs)\n",
    "onecycle = OneCycleScheduler(len(X_train_resize) // batch_size * epochs, max_rate=0.003)\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=f\"smv2l_adam_best_lrsch002_onecycle_aug_dropout07_128_he_irb6_512_t4_epoch{epochs}_noact.h5\",save_best_only=True)\n",
    "\n",
    "history=smv2_l.fit(X_train,y_train,\n",
    "                   validation_data=(X_valid_final,y_valid),batch_size = batch_size, \n",
    "                   epochs=epochs,callbacks=[tboard_callback, check_best_cb,onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce750a",
   "metadata": {},
   "source": [
    "마지막 layer activation 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0d72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 16, 16, 32)\n",
      "IRB2 : batch_input_shape = (None, 16, 16, 16)\n",
      "IRB3 : batch_input_shape = (None, 8, 8, 16)\n",
      "IRB4 : batch_input_shape = (None, 4, 4, 32)\n",
      "IRB5 : batch_input_shape = (None, 2, 2, 64)\n",
      "IRB6 : batch_input_shape = (None, 1, 1, 128)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 32, 32, 1)        4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " IRB1 (Inveted_Residual_Bloc  (None, 16, 16, 16)       2288      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB2 (Inveted_Residual_Bloc  (None, 8, 8, 16)         3424      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB3 (Inveted_Residual_Bloc  (None, 4, 4, 32)         9856      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB4 (Inveted_Residual_Bloc  (None, 2, 2, 64)         34048     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB5 (Inveted_Residual_Bloc  (None, 1, 1, 128)        54400     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB6 (Inveted_Residual_Bloc  (None, 1, 1, 1024)       601088    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                63550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 773,074\n",
      "Trainable params: 765,232\n",
      "Non-trainable params: 7,842\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #황성현 전민재 IRB61024, maxlr 0.003\n",
    "    #multygpu\n",
    "\n",
    "\n",
    "    n_classes=62\n",
    "    smv2_l=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[32,32,1]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=2,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=4,c=32,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=4,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=4,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=4,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "        \n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_l.summary()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    batch_size=128\n",
    "    smv2_l.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a37376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:58:16.126621: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-09 01:58:16.126671: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-06-09 01:58:16.126739: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 4 GPUs\n",
      "2022-06-09 01:58:16.130279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-06-09 01:58:16.897455: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-09 01:58:16.897675: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-09 01:58:16.990824: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n",
      "2022-06-09 01:58:19.960463: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:58:24.490574: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 499/8393 [>.............................] - ETA: 3:21 - loss: 4.8703 - accuracy: 0.0423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:58:38.763771: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-09 01:58:38.763828: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 518/8393 [>.............................] - ETA: 3:36 - loss: 4.8510 - accuracy: 0.0433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:58:40.285731: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-06-09 01:58:40.287166: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-09 01:58:40.487802: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 14680 callback api events and 14753 activity events. \n",
      "2022-06-09 01:58:40.736842: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-09 01:58:41.048911: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40\n",
      "\n",
      "2022-06-09 01:58:41.278376: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40/a58a2589147d.trace.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 525/8393 [>.............................] - ETA: 3:55 - loss: 4.8441 - accuracy: 0.0435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 01:58:41.549734: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40\n",
      "\n",
      "2022-06-09 01:58:41.556964: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40/a58a2589147d.memory_profile.json.gz\n",
      "2022-06-09 01:58:41.562301: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40\n",
      "Dumped tool data for xplane.pb to logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40/a58a2589147d.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40/a58a2589147d.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40/a58a2589147d.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40/a58a2589147d.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch4020220609-015816/plugins/profile/2022_06_09_01_58_40/a58a2589147d.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8393/8393 [==============================] - ETA: 0s - loss: 1.6326 - accuracy: 0.5595lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00045000098>\n",
      "8393/8393 [==============================] - 214s 25ms/step - loss: 1.6326 - accuracy: 0.5595 - val_loss: 0.4990 - val_accuracy: 0.8211\n",
      "Epoch 2/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.8169lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0006000199>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.5140 - accuracy: 0.8169 - val_loss: 0.4271 - val_accuracy: 0.8436\n",
      "Epoch 3/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4528 - accuracy: 0.8355lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0007500387>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.4528 - accuracy: 0.8355 - val_loss: 0.4111 - val_accuracy: 0.8503\n",
      "Epoch 4/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.4264 - accuracy: 0.8435lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009000576>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.4264 - accuracy: 0.8435 - val_loss: 0.4050 - val_accuracy: 0.8489\n",
      "Epoch 5/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.8482lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0010500764>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.4111 - accuracy: 0.8482 - val_loss: 0.4003 - val_accuracy: 0.8518\n",
      "Epoch 6/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8514lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0012000953>\n",
      "8393/8393 [==============================] - 206s 24ms/step - loss: 0.4015 - accuracy: 0.8514 - val_loss: 0.3955 - val_accuracy: 0.8548\n",
      "Epoch 7/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8535lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0013501142>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3956 - accuracy: 0.8535 - val_loss: 0.4521 - val_accuracy: 0.8351\n",
      "Epoch 8/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3917 - accuracy: 0.8545lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0015001331>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.3918 - accuracy: 0.8545 - val_loss: 0.3883 - val_accuracy: 0.8516\n",
      "Epoch 9/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8552lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016501519>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3893 - accuracy: 0.8552 - val_loss: 0.4336 - val_accuracy: 0.8398\n",
      "Epoch 10/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3882 - accuracy: 0.8556lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018001708>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3882 - accuracy: 0.8556 - val_loss: 0.4952 - val_accuracy: 0.8215\n",
      "Epoch 11/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8559lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0019501897>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.3869 - accuracy: 0.8559 - val_loss: 0.4147 - val_accuracy: 0.8431\n",
      "Epoch 12/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8562lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0021002085>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.3867 - accuracy: 0.8562 - val_loss: 1.1946 - val_accuracy: 0.6938\n",
      "Epoch 13/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8559lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022502274>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.3867 - accuracy: 0.8559 - val_loss: 1.0248 - val_accuracy: 0.7300\n",
      "Epoch 14/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3879 - accuracy: 0.8559lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0024002462>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3879 - accuracy: 0.8559 - val_loss: 0.4126 - val_accuracy: 0.8516\n",
      "Epoch 15/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8555lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002550265>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3892 - accuracy: 0.8555 - val_loss: 1.8424 - val_accuracy: 0.5789\n",
      "Epoch 16/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3905 - accuracy: 0.8548lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002700284>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3905 - accuracy: 0.8548 - val_loss: 0.5900 - val_accuracy: 0.7974\n",
      "Epoch 17/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3937 - accuracy: 0.8543lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002850303>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3937 - accuracy: 0.8543 - val_loss: 0.6913 - val_accuracy: 0.7959\n",
      "Epoch 18/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8534lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0029996783>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3976 - accuracy: 0.8534 - val_loss: 0.5739 - val_accuracy: 0.8070\n",
      "Epoch 19/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.8535lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028496594>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3972 - accuracy: 0.8535 - val_loss: 0.4375 - val_accuracy: 0.8370\n",
      "Epoch 20/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.8553lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0026996406>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3917 - accuracy: 0.8553 - val_loss: 0.3987 - val_accuracy: 0.8521\n",
      "Epoch 21/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3859 - accuracy: 0.8567lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0025496217>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.3859 - accuracy: 0.8567 - val_loss: 0.5386 - val_accuracy: 0.8135\n",
      "Epoch 22/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.8586lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002399603>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.3803 - accuracy: 0.8586 - val_loss: 0.3975 - val_accuracy: 0.8568\n",
      "Epoch 23/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3754 - accuracy: 0.8601lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022495838>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.3754 - accuracy: 0.8601 - val_loss: 0.4134 - val_accuracy: 0.8436\n",
      "Epoch 24/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8619lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002099565>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3692 - accuracy: 0.8619 - val_loss: 0.6664 - val_accuracy: 0.7839\n",
      "Epoch 25/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.8640lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0019495462>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3628 - accuracy: 0.8640 - val_loss: 0.7080 - val_accuracy: 0.7930\n",
      "Epoch 26/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.8656lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0017995273>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3569 - accuracy: 0.8656 - val_loss: 0.3673 - val_accuracy: 0.8625\n",
      "Epoch 27/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8671lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016495085>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3512 - accuracy: 0.8671 - val_loss: 0.3633 - val_accuracy: 0.8677\n",
      "Epoch 28/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8686lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0014994895>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3454 - accuracy: 0.8686 - val_loss: 0.8558 - val_accuracy: 0.7749\n",
      "Epoch 29/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8708lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0013494707>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3393 - accuracy: 0.8708 - val_loss: 0.3810 - val_accuracy: 0.8578\n",
      "Epoch 30/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8725lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0011994519>\n",
      "8393/8393 [==============================] - 207s 25ms/step - loss: 0.3332 - accuracy: 0.8725 - val_loss: 0.3516 - val_accuracy: 0.8711\n",
      "Epoch 31/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.8741lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001049433>\n",
      "8393/8393 [==============================] - 208s 25ms/step - loss: 0.3272 - accuracy: 0.8741 - val_loss: 0.3721 - val_accuracy: 0.8622\n",
      "Epoch 32/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.8760lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008994141>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3216 - accuracy: 0.8760 - val_loss: 0.4728 - val_accuracy: 0.8343\n",
      "Epoch 33/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8777lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0007493952>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3154 - accuracy: 0.8777 - val_loss: 0.3387 - val_accuracy: 0.8736\n",
      "Epoch 34/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.8797lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0005993764>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.3095 - accuracy: 0.8797 - val_loss: 0.3725 - val_accuracy: 0.8623\n",
      "Epoch 35/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.8816lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00044935753>\n",
      "8393/8393 [==============================] - 206s 24ms/step - loss: 0.3033 - accuracy: 0.8816 - val_loss: 0.3960 - val_accuracy: 0.8562\n",
      "Epoch 36/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8837lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0002996697>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.2978 - accuracy: 0.8837 - val_loss: 0.8827 - val_accuracy: 0.7917\n",
      "Epoch 37/40\n",
      "8391/8393 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8848lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00022474022>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.2927 - accuracy: 0.8848 - val_loss: 0.3321 - val_accuracy: 0.8770\n",
      "Epoch 38/40\n",
      "8392/8393 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8860lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00014981074>\n",
      "8393/8393 [==============================] - 209s 25ms/step - loss: 0.2897 - accuracy: 0.8860 - val_loss: 0.3336 - val_accuracy: 0.8771\n",
      "Epoch 39/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.8871lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.488129e-05>\n",
      "8393/8393 [==============================] - 206s 25ms/step - loss: 0.2863 - accuracy: 0.8871 - val_loss: 0.3293 - val_accuracy: 0.8781\n",
      "Epoch 40/40\n",
      "8393/8393 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.8879lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=3e-07>\n",
      "8393/8393 [==============================] - 205s 24ms/step - loss: 0.2841 - accuracy: 0.8879 - val_loss: 0.3295 - val_accuracy: 0.8786\n"
     ]
    }
   ],
   "source": [
    "#황성현 전민재\n",
    "from datetime import datetime\n",
    "K = keras.backend\n",
    "epochs=40\n",
    "logs = \"logs/\" + datetime.now().strftime(\"SMV2L_lrsch_onecycle_dropout50_128_he_irb6_1024_epoch40%Y%m%d-%H%M%S\")\n",
    "onecycle = OneCycleScheduler(len(X_train_resize) // batch_size * epochs, max_rate=0.003,start_rate=0.0003)\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2l_adam_best_lrsch_onecycle_aug_dropout50_128_he_irb6_1024_t4_epoch40.h5\",save_best_only=True)\n",
    "\n",
    "history=smv2_l.fit(X_train,y_train,\n",
    "                   validation_data=(X_valid_final,y_valid),batch_size = batch_size, \n",
    "                   epochs=epochs,callbacks=[tboard_callback, check_best_cb,onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96bc5ab0-c347-44c5-880c-cf28cf554299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRB1 : batch_input_shape = (None, 16, 16, 32)\n",
      "IRB2 : batch_input_shape = (None, 16, 16, 16)\n",
      "IRB3 : batch_input_shape = (None, 8, 8, 16)\n",
      "IRB4 : batch_input_shape = (None, 4, 4, 32)\n",
      "IRB5 : batch_input_shape = (None, 2, 2, 64)\n",
      "IRB6 : batch_input_shape = (None, 1, 1, 128)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 32, 32, 1)        4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " IRB1 (Inveted_Residual_Bloc  (None, 16, 16, 16)       2288      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB2 (Inveted_Residual_Bloc  (None, 8, 8, 16)         3424      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB3 (Inveted_Residual_Bloc  (None, 4, 4, 32)         9856      \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB4 (Inveted_Residual_Bloc  (None, 2, 2, 64)         34048     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB5 (Inveted_Residual_Bloc  (None, 1, 1, 128)        54400     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " IRB6 (Inveted_Residual_Bloc  (None, 1, 1, 1024)       601088    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " re_lu_13 (ReLU)             (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                63550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 773,074\n",
      "Trainable params: 765,232\n",
      "Non-trainable params: 7,842\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #황성현 전민재 IRB61024, maxlr 0.003\n",
    "    #multygpu\n",
    "\n",
    "\n",
    "    n_classes=62\n",
    "    smv2_l=keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=[32,32,1]),\n",
    "        keras.layers.Conv2D(filters=32,kernel_size=3,strides=2,padding=\"same\",activation=\"relu\"),\n",
    "        Inveted_Residual_Block(t=1,c=16,n=1,s=1,name=\"IRB1\"),\n",
    "        Inveted_Residual_Block(t=2,c=16,n=2,s=2,name=\"IRB2\"),\n",
    "        Inveted_Residual_Block(t=4,c=32,n=2,s=2,name=\"IRB3\"),\n",
    "        Inveted_Residual_Block(t=4,c=64,n=2,s=2,name=\"IRB4\"),\n",
    "        Inveted_Residual_Block(t=4,c=128,n=1,s=2,name=\"IRB5\"),\n",
    "        Inveted_Residual_Block(t=4,c=1024,n=1,s=1,name=\"IRB6\"),\n",
    "        keras.layers.ReLU(max_value=6),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(n_classes,activation=\"softmax\")\n",
    "    ])\n",
    "    smv2_l.summary()\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    batch_size=32\n",
    "    smv2_l.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c2c4d-51a3-46f1-8120-a396454c0bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 05:37:19.292208: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-09 05:37:19.292237: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-06-09 05:37:20.120829: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-09 05:37:20.121055: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-09 05:37:20.226198: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n",
      "2022-06-09 05:37:23.274603: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4400181248 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "    5/33571 [..............................] - ETA: 9:48 - loss: 5.6660 - accuracy: 0.0250    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0168s vs `on_train_batch_end` time: 0.0380s). Check your callbacks.\n",
      "  499/33571 [..............................] - ETA: 12:21 - loss: 4.4551 - accuracy: 0.0889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 05:37:38.707555: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-09 05:37:38.707614: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  518/33571 [..............................] - ETA: 13:20 - loss: 4.4274 - accuracy: 0.0906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 05:37:40.132170: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-06-09 05:37:40.133478: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-09 05:37:40.323830: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 14050 callback api events and 14115 activity events. \n",
      "2022-06-09 05:37:40.563236: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-09 05:37:40.854569: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40\n",
      "\n",
      "2022-06-09 05:37:41.065467: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40/a58a2589147d.trace.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  526/33571 [..............................] - ETA: 14:38 - loss: 4.4146 - accuracy: 0.0918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 05:37:41.318784: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40\n",
      "\n",
      "2022-06-09 05:37:41.326250: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40/a58a2589147d.memory_profile.json.gz\n",
      "2022-06-09 05:37:41.331255: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40\n",
      "Dumped tool data for xplane.pb to logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40/a58a2589147d.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40/a58a2589147d.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40/a58a2589147d.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40/a58a2589147d.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch4020220609-053719/plugins/profile/2022_06_09_05_37_40/a58a2589147d.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33571/33571 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.7399lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00033888896>\n",
      "33571/33571 [==============================] - 797s 24ms/step - loss: 0.8280 - accuracy: 0.7399 - val_loss: 0.4291 - val_accuracy: 0.8424\n",
      "Epoch 2/40\n",
      "33569/33571 [============================>.] - ETA: 0s - loss: 0.4834 - accuracy: 0.8272lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00037777907>\n",
      "33571/33571 [==============================] - 784s 23ms/step - loss: 0.4834 - accuracy: 0.8272 - val_loss: 0.4258 - val_accuracy: 0.8431\n",
      "Epoch 3/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.4458 - accuracy: 0.8382lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00041666918>\n",
      "33571/33571 [==============================] - 787s 23ms/step - loss: 0.4458 - accuracy: 0.8382 - val_loss: 0.3848 - val_accuracy: 0.8572\n",
      "Epoch 4/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8442lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0004555593>\n",
      "33571/33571 [==============================] - 787s 23ms/step - loss: 0.4255 - accuracy: 0.8442 - val_loss: 0.3721 - val_accuracy: 0.8620\n",
      "Epoch 5/40\n",
      "33569/33571 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8481lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0004944494>\n",
      "33571/33571 [==============================] - 788s 23ms/step - loss: 0.4135 - accuracy: 0.8481 - val_loss: 0.3672 - val_accuracy: 0.8628\n",
      "Epoch 6/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8508lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0005333395>\n",
      "33571/33571 [==============================] - 786s 23ms/step - loss: 0.4045 - accuracy: 0.8508 - val_loss: 0.3615 - val_accuracy: 0.8648\n",
      "Epoch 7/40\n",
      "33570/33571 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8530lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0005722296>\n",
      "33571/33571 [==============================] - 789s 24ms/step - loss: 0.3981 - accuracy: 0.8530 - val_loss: 0.3582 - val_accuracy: 0.8667\n",
      "Epoch 8/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8544lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0006111197>\n",
      "33571/33571 [==============================] - 741s 22ms/step - loss: 0.3927 - accuracy: 0.8544 - val_loss: 0.3671 - val_accuracy: 0.8610\n",
      "Epoch 9/40\n",
      "33569/33571 [============================>.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8552lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00065000984>\n",
      "33571/33571 [==============================] - 676s 20ms/step - loss: 0.3895 - accuracy: 0.8552 - val_loss: 0.3675 - val_accuracy: 0.8610\n",
      "Epoch 10/40\n",
      "33568/33571 [============================>.] - ETA: 0s - loss: 0.3860 - accuracy: 0.8565lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00068889995>\n",
      "33571/33571 [==============================] - 685s 20ms/step - loss: 0.3860 - accuracy: 0.8565 - val_loss: 0.3539 - val_accuracy: 0.8680\n",
      "Epoch 11/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.8570lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00072779006>\n",
      "33571/33571 [==============================] - 679s 20ms/step - loss: 0.3835 - accuracy: 0.8570 - val_loss: 0.3557 - val_accuracy: 0.8667\n",
      "Epoch 12/40\n",
      "33570/33571 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.8576lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00076668017>\n",
      "33571/33571 [==============================] - 683s 20ms/step - loss: 0.3816 - accuracy: 0.8576 - val_loss: 0.3528 - val_accuracy: 0.8686\n",
      "Epoch 13/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3794 - accuracy: 0.8585lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008055703>\n",
      "33571/33571 [==============================] - 679s 20ms/step - loss: 0.3794 - accuracy: 0.8585 - val_loss: 0.3544 - val_accuracy: 0.8676\n",
      "Epoch 14/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.8591lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008444604>\n",
      "33571/33571 [==============================] - 685s 20ms/step - loss: 0.3774 - accuracy: 0.8591 - val_loss: 0.3536 - val_accuracy: 0.8684\n",
      "Epoch 15/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.8599lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008833505>\n",
      "33571/33571 [==============================] - 677s 20ms/step - loss: 0.3757 - accuracy: 0.8599 - val_loss: 0.3589 - val_accuracy: 0.8686\n",
      "Epoch 16/40\n",
      "33569/33571 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8598lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009222406>\n",
      "33571/33571 [==============================] - 684s 20ms/step - loss: 0.3753 - accuracy: 0.8598 - val_loss: 0.3597 - val_accuracy: 0.8655\n",
      "Epoch 17/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3744 - accuracy: 0.8596lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009611307>\n",
      "33571/33571 [==============================] - 679s 20ms/step - loss: 0.3744 - accuracy: 0.8596 - val_loss: 0.3499 - val_accuracy: 0.8702\n",
      "Epoch 18/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3733 - accuracy: 0.8604lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009999791>\n",
      "33571/33571 [==============================] - 681s 20ms/step - loss: 0.3733 - accuracy: 0.8604 - val_loss: 0.3559 - val_accuracy: 0.8648\n",
      "Epoch 19/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.8612lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00096108904>\n",
      "33571/33571 [==============================] - 680s 20ms/step - loss: 0.3707 - accuracy: 0.8612 - val_loss: 0.3487 - val_accuracy: 0.8705\n",
      "Epoch 20/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.8628lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00092219893>\n",
      "33571/33571 [==============================] - 682s 20ms/step - loss: 0.3658 - accuracy: 0.8628 - val_loss: 0.3566 - val_accuracy: 0.8649\n",
      "Epoch 21/40\n",
      "33570/33571 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8644lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008833088>\n",
      "33571/33571 [==============================] - 683s 20ms/step - loss: 0.3611 - accuracy: 0.8644 - val_loss: 0.3414 - val_accuracy: 0.8719\n",
      "Epoch 22/40\n",
      "33570/33571 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8655lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008444187>\n",
      "33571/33571 [==============================] - 678s 20ms/step - loss: 0.3568 - accuracy: 0.8655 - val_loss: 0.3441 - val_accuracy: 0.8713\n",
      "Epoch 23/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3533 - accuracy: 0.8666lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008055286>\n",
      "33571/33571 [==============================] - 688s 20ms/step - loss: 0.3533 - accuracy: 0.8666 - val_loss: 0.3392 - val_accuracy: 0.8728\n",
      "Epoch 24/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3489 - accuracy: 0.8680lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0007666385>\n",
      "33571/33571 [==============================] - 683s 20ms/step - loss: 0.3489 - accuracy: 0.8680 - val_loss: 0.3477 - val_accuracy: 0.8677\n",
      "Epoch 25/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8690lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0007277484>\n",
      "33571/33571 [==============================] - 686s 20ms/step - loss: 0.3455 - accuracy: 0.8690 - val_loss: 0.3352 - val_accuracy: 0.8751\n",
      "Epoch 26/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.8698lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0006888583>\n",
      "33571/33571 [==============================] - 685s 20ms/step - loss: 0.3418 - accuracy: 0.8698 - val_loss: 0.3371 - val_accuracy: 0.8732\n",
      "Epoch 27/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.8710lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00064996816>\n",
      "33571/33571 [==============================] - 687s 20ms/step - loss: 0.3386 - accuracy: 0.8710 - val_loss: 0.4799 - val_accuracy: 0.8331\n",
      "Epoch 28/40\n",
      "33570/33571 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8719lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00061107805>\n",
      "33571/33571 [==============================] - 686s 20ms/step - loss: 0.3349 - accuracy: 0.8719 - val_loss: 0.3425 - val_accuracy: 0.8726\n",
      "Epoch 29/40\n",
      "33569/33571 [============================>.] - ETA: 0s - loss: 0.3320 - accuracy: 0.8729lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00057218794>\n",
      "33571/33571 [==============================] - 690s 21ms/step - loss: 0.3320 - accuracy: 0.8729 - val_loss: 0.3302 - val_accuracy: 0.8759\n",
      "Epoch 30/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.8740lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00053329783>\n",
      "33571/33571 [==============================] - 691s 21ms/step - loss: 0.3288 - accuracy: 0.8740 - val_loss: 0.3345 - val_accuracy: 0.8751\n",
      "Epoch 31/40\n",
      "33568/33571 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8747lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0004944077>\n",
      "33571/33571 [==============================] - 688s 20ms/step - loss: 0.3255 - accuracy: 0.8747 - val_loss: 0.3331 - val_accuracy: 0.8744\n",
      "Epoch 32/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.8759lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00045551758>\n",
      "33571/33571 [==============================] - 688s 20ms/step - loss: 0.3223 - accuracy: 0.8759 - val_loss: 0.3282 - val_accuracy: 0.8770\n",
      "Epoch 33/40\n",
      "33570/33571 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8769lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00041662747>\n",
      "33571/33571 [==============================] - 686s 20ms/step - loss: 0.3191 - accuracy: 0.8769 - val_loss: 0.3338 - val_accuracy: 0.8743\n",
      "Epoch 34/40\n",
      "33567/33571 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8776lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00037773736>\n",
      "33571/33571 [==============================] - 686s 20ms/step - loss: 0.3165 - accuracy: 0.8776 - val_loss: 0.3294 - val_accuracy: 0.8758\n",
      "Epoch 35/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3137 - accuracy: 0.8786lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00033884725>\n",
      "33571/33571 [==============================] - 689s 21ms/step - loss: 0.3137 - accuracy: 0.8786 - val_loss: 0.3273 - val_accuracy: 0.8776\n",
      "Epoch 36/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.8792lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00029991742>\n",
      "33571/33571 [==============================] - 689s 21ms/step - loss: 0.3109 - accuracy: 0.8792 - val_loss: 0.3259 - val_accuracy: 0.8777\n",
      "Epoch 37/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.8805lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00022499131>\n",
      "33571/33571 [==============================] - 676s 20ms/step - loss: 0.3073 - accuracy: 0.8805 - val_loss: 0.3278 - val_accuracy: 0.8775\n",
      "Epoch 38/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8819lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00015006519>\n",
      "33571/33571 [==============================] - 676s 20ms/step - loss: 0.3026 - accuracy: 0.8819 - val_loss: 0.3238 - val_accuracy: 0.8794\n",
      "Epoch 39/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.8834lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.5139076e-05>\n",
      "33571/33571 [==============================] - 676s 20ms/step - loss: 0.2981 - accuracy: 0.8834 - val_loss: 0.3226 - val_accuracy: 0.8800\n",
      "Epoch 40/40\n",
      "33571/33571 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.8844"
     ]
    }
   ],
   "source": [
    "#황성현 전민재\n",
    "from datetime import datetime\n",
    "K = keras.backend\n",
    "epochs=40\n",
    "logs = \"logs/\" + datetime.now().strftime(\"SMV2L_lrsch_onecycle001_dropout_128_he_irb6_1024_epoch40%Y%m%d-%H%M%S\")\n",
    "onecycle = OneCycleScheduler(len(X_train_resize) // batch_size * epochs, max_rate=0.001,start_rate=0.0003)\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "check_best_cb=tf.keras.callbacks.ModelCheckpoint(filepath=\"smv2l_adam_best_lrsch_onecycle001_aug_dropout50_128_he_irb6_1024_t4_epoch40.h5\",save_best_only=True)\n",
    "\n",
    "history=smv2_l.fit(X_train,y_train,\n",
    "                   validation_data=(X_valid_final,y_valid),batch_size = batch_size, \n",
    "                   epochs=epochs,callbacks=[tboard_callback, check_best_cb,onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d31b0-e2b7-4b52-93be-41b80de466f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smv2_l.save('./smv2l_adam_lrsch_onecycle001_aug_dropout_128_he_irb6_512_noact.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166252fc-8fa0-4882-949f-6ac8aab40ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
